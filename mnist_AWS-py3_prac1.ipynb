{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a quick tour for going end-to-end model building and tuning for MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import utils\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (60000,), (10000, 28, 28), (10000,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remember this step is necessary for MNIST data\n",
    "X_train = np.expand_dims(X_train, 1) # np.expand_dims(X_train, 3) # for channel last case\n",
    "X_test = np.expand_dims(X_test, 1) # np.expand_dims(X_test, 3) # for channel last case "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 1, 28, 28), (10000, 1, 28, 28))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, 1, 9], dtype=uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### turn y_train, y_test from classes into labels by onehot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = onehot(y_train)\n",
    "y_test = onehot(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_px = X_train.mean().astype(np.float32)\n",
    "std_px = X_train.std().astype(np.float32)\n",
    "\n",
    "def norm_input(x): return (x - mean_px) / std_px "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and Tune Model (to overfitting)\n",
    "** tune it to overfitting to make sure that your model is complex enough to work well **\n",
    "\n",
    "** after making it, then we can tune it to reduce overfitting for the next steps by different methods **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lin_model():\n",
    "    model = Sequential([\n",
    "        Lambda(norm_input, input_shape=(1, 28, 28)),\n",
    "        Flatten(),\n",
    "        Dense(10, activation='softmax')\n",
    "        ])\n",
    "    model.compile(Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = get_lin_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen = image.ImageDataGenerator()\n",
    "batches = gen.flow(X_train, y_train, batch_size=batch_size)\n",
    "test_batches = gen.flow(X_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "\n",
      "  1/938 [..............................] - ETA: 4:37 - loss: 2.7860 - acc: 0.2500\n",
      " 16/938 [..............................] - ETA: 20s - loss: 2.1186 - acc: 0.3252 \n",
      " 31/938 [..............................] - ETA: 11s - loss: 1.6245 - acc: 0.4934\n",
      " 46/938 [>.............................] - ETA: 8s - loss: 1.3593 - acc: 0.5812 \n",
      " 61/938 [>.............................] - ETA: 7s - loss: 1.1855 - acc: 0.6352\n",
      " 76/938 [=>............................] - ETA: 6s - loss: 1.0723 - acc: 0.6715\n",
      " 91/938 [=>............................] - ETA: 5s - loss: 0.9777 - acc: 0.7026\n",
      "106/938 [==>...........................] - ETA: 5s - loss: 0.9087 - acc: 0.7248\n",
      "121/938 [==>...........................] - ETA: 4s - loss: 0.8568 - acc: 0.7401\n",
      "137/938 [===>..........................] - ETA: 4s - loss: 0.8095 - acc: 0.7540\n",
      "153/938 [===>..........................] - ETA: 4s - loss: 0.7712 - acc: 0.7660\n",
      "169/938 [====>.........................] - ETA: 3s - loss: 0.7437 - acc: 0.7746\n",
      "184/938 [====>.........................] - ETA: 3s - loss: 0.7185 - acc: 0.7835\n",
      "199/938 [=====>........................] - ETA: 3s - loss: 0.6964 - acc: 0.7906\n",
      "215/938 [=====>........................] - ETA: 3s - loss: 0.6772 - acc: 0.7959\n",
      "231/938 [======>.......................] - ETA: 3s - loss: 0.6579 - acc: 0.8024\n",
      "246/938 [======>.......................] - ETA: 3s - loss: 0.6420 - acc: 0.8072\n",
      "261/938 [=======>......................] - ETA: 3s - loss: 0.6288 - acc: 0.8118\n",
      "277/938 [=======>......................] - ETA: 2s - loss: 0.6124 - acc: 0.8168\n",
      "292/938 [========>.....................] - ETA: 2s - loss: 0.6012 - acc: 0.8200\n",
      "308/938 [========>.....................] - ETA: 2s - loss: 0.5907 - acc: 0.8232\n",
      "324/938 [=========>....................] - ETA: 2s - loss: 0.5804 - acc: 0.8258\n",
      "339/938 [=========>....................] - ETA: 2s - loss: 0.5701 - acc: 0.8291\n",
      "355/938 [==========>...................] - ETA: 2s - loss: 0.5607 - acc: 0.8320\n",
      "370/938 [==========>...................] - ETA: 2s - loss: 0.5520 - acc: 0.8347\n",
      "385/938 [===========>..................] - ETA: 2s - loss: 0.5435 - acc: 0.8375\n",
      "400/938 [===========>..................] - ETA: 2s - loss: 0.5365 - acc: 0.8391\n",
      "415/938 [============>.................] - ETA: 2s - loss: 0.5293 - acc: 0.8415\n",
      "431/938 [============>.................] - ETA: 2s - loss: 0.5223 - acc: 0.8441\n",
      "446/938 [=============>................] - ETA: 1s - loss: 0.5161 - acc: 0.8461\n",
      "462/938 [=============>................] - ETA: 1s - loss: 0.5117 - acc: 0.8476\n",
      "478/938 [==============>...............] - ETA: 1s - loss: 0.5060 - acc: 0.8492\n",
      "494/938 [==============>...............] - ETA: 1s - loss: 0.5029 - acc: 0.8506\n",
      "510/938 [===============>..............] - ETA: 1s - loss: 0.4978 - acc: 0.8523\n",
      "526/938 [===============>..............] - ETA: 1s - loss: 0.4931 - acc: 0.8537\n",
      "542/938 [================>.............] - ETA: 1s - loss: 0.4873 - acc: 0.8555\n",
      "558/938 [================>.............] - ETA: 1s - loss: 0.4823 - acc: 0.8570\n",
      "574/938 [=================>............] - ETA: 1s - loss: 0.4780 - acc: 0.8584\n",
      "590/938 [=================>............] - ETA: 1s - loss: 0.4743 - acc: 0.8597\n",
      "606/938 [==================>...........] - ETA: 1s - loss: 0.4699 - acc: 0.8608\n",
      "622/938 [==================>...........] - ETA: 1s - loss: 0.4662 - acc: 0.8622\n",
      "638/938 [===================>..........] - ETA: 1s - loss: 0.4617 - acc: 0.8634\n",
      "654/938 [===================>..........] - ETA: 1s - loss: 0.4580 - acc: 0.8645\n",
      "670/938 [====================>.........] - ETA: 1s - loss: 0.4547 - acc: 0.8659\n",
      "686/938 [====================>.........] - ETA: 0s - loss: 0.4526 - acc: 0.8665\n",
      "702/938 [=====================>........] - ETA: 0s - loss: 0.4506 - acc: 0.8669\n",
      "718/938 [=====================>........] - ETA: 0s - loss: 0.4477 - acc: 0.8677\n",
      "734/938 [======================>.......] - ETA: 0s - loss: 0.4454 - acc: 0.8684\n",
      "750/938 [======================>.......] - ETA: 0s - loss: 0.4433 - acc: 0.8691\n",
      "765/938 [=======================>......] - ETA: 0s - loss: 0.4418 - acc: 0.8700\n",
      "781/938 [=======================>......] - ETA: 0s - loss: 0.4390 - acc: 0.8707\n",
      "796/938 [========================>.....] - ETA: 0s - loss: 0.4367 - acc: 0.8713\n",
      "812/938 [========================>.....] - ETA: 0s - loss: 0.4342 - acc: 0.8719\n",
      "828/938 [=========================>....] - ETA: 0s - loss: 0.4327 - acc: 0.8725\n",
      "844/938 [=========================>....] - ETA: 0s - loss: 0.4312 - acc: 0.8730\n",
      "859/938 [==========================>...] - ETA: 0s - loss: 0.4300 - acc: 0.8734\n",
      "874/938 [==========================>...] - ETA: 0s - loss: 0.4286 - acc: 0.8737\n",
      "889/938 [===========================>..] - ETA: 0s - loss: 0.4261 - acc: 0.8743\n",
      "905/938 [===========================>..] - ETA: 0s - loss: 0.4239 - acc: 0.8750\n",
      "920/938 [============================>.] - ETA: 0s - loss: 0.4227 - acc: 0.8756\n",
      "936/938 [============================>.] - ETA: 0s - loss: 0.4212 - acc: 0.8758\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.4211 - acc: 0.8758 - val_loss: 0.3058 - val_acc: 0.9112\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb945aad470>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.fit_generator(batches, steps_per_epoch=int(math.ceil(batches.n / batches.batch_size)), epochs=1, verbose=2,\n",
    "                 validation_data=test_batches, validation_steps=int(math.ceil(test_batches.n/test_batches.batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lm.optimizer.lr = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      " - 3s - loss: 0.2987 - acc: 0.9146 - val_loss: 0.2822 - val_acc: 0.9183\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb945aad240>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.fit_generator(batches, steps_per_epoch=int(math.ceil(batches.n / batches.batch_size)), epochs=1, verbose=2,\n",
    "                 validation_data=test_batches, validation_steps=int(math.ceil(test_batches.n/test_batches.batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lm.optimizer.lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      " - 3s - loss: 0.2850 - acc: 0.9199 - val_loss: 0.2810 - val_acc: 0.9180\n",
      "Epoch 2/4\n",
      " - 3s - loss: 0.2772 - acc: 0.9221 - val_loss: 0.2905 - val_acc: 0.9190\n",
      "Epoch 3/4\n",
      " - 3s - loss: 0.2734 - acc: 0.9237 - val_loss: 0.2795 - val_acc: 0.9233\n",
      "Epoch 4/4\n",
      " - 3s - loss: 0.2692 - acc: 0.9242 - val_loss: 0.2814 - val_acc: 0.9218\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb945aad518>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.fit_generator(batches, steps_per_epoch=int(math.ceil(batches.n / batches.batch_size)), epochs=3, verbose=2,\n",
    "                 validation_data=test_batches, validation_steps=int(math.ceil(test_batches.n/test_batches.batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG style model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_bn():\n",
    "    model = Sequential([\n",
    "        Lambda(norm_input, input_shape=(1, 28, 28)),\n",
    "        Conv2D(32, (3,3), activation='relu'),\n",
    "        BatchNormalization(axis=1),\n",
    "        Conv2D(32, (3,3), activation='relu'),\n",
    "        MaxPooling2D(),\n",
    "        BatchNormalization(axis=1),\n",
    "        Conv2D(64, (3,3), activation='relu'),\n",
    "        BatchNormalization(axis=1),\n",
    "        Conv2D(64, (3,3), activation='relu'),\n",
    "        MaxPooling2D(),\n",
    "        Flatten(),\n",
    "        BatchNormalization(),\n",
    "        Dense(512, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = get_model_bn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      " - 104s - loss: 0.0855 - acc: 0.9743 - val_loss: 0.0410 - val_acc: 0.9870\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb9443d9ac8>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(batches, steps_per_epoch=int(math.ceil(batches.n / batches.batch_size)), epochs=1, verbose=2,\n",
    "                 validation_data=test_batches, validation_steps=int(math.ceil(test_batches.n/test_batches.batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      " - 102s - loss: 0.0348 - acc: 0.9889 - val_loss: 0.0321 - val_acc: 0.9890\n",
      "Epoch 2/8\n",
      " - 102s - loss: 0.0246 - acc: 0.9919 - val_loss: 0.0280 - val_acc: 0.9914\n",
      "Epoch 3/8\n",
      " - 102s - loss: 0.0221 - acc: 0.9927 - val_loss: 0.0279 - val_acc: 0.9913\n",
      "Epoch 4/8\n",
      " - 102s - loss: 0.0183 - acc: 0.9940 - val_loss: 0.0290 - val_acc: 0.9911\n",
      "Epoch 5/8\n",
      " - 102s - loss: 0.0166 - acc: 0.9947 - val_loss: 0.0300 - val_acc: 0.9921\n",
      "Epoch 6/8\n",
      " - 102s - loss: 0.0152 - acc: 0.9951 - val_loss: 0.0270 - val_acc: 0.9914\n",
      "Epoch 7/8\n",
      " - 102s - loss: 0.0121 - acc: 0.9959 - val_loss: 0.0269 - val_acc: 0.9910\n",
      "Epoch 8/8\n",
      " - 102s - loss: 0.0112 - acc: 0.9960 - val_loss: 0.0366 - val_acc: 0.9917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb9443d99e8>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(batches, steps_per_epoch=int(math.ceil(batches.n / batches.batch_size)), epochs=2, verbose=2,\n",
    "                 validation_data=test_batches, validation_steps=int(math.ceil(test_batches.n/test_batches.batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      " - 102s - loss: 0.0096 - acc: 0.9970 - val_loss: 0.0375 - val_acc: 0.9918\n",
      "Epoch 2/12\n",
      " - 102s - loss: 0.0084 - acc: 0.9973 - val_loss: 0.0300 - val_acc: 0.9933\n",
      "Epoch 3/12\n",
      " - 102s - loss: 0.0075 - acc: 0.9975 - val_loss: 0.0306 - val_acc: 0.9912\n",
      "Epoch 4/12\n",
      " - 102s - loss: 0.0067 - acc: 0.9979 - val_loss: 0.0376 - val_acc: 0.9910\n",
      "Epoch 5/12\n",
      " - 102s - loss: 0.0079 - acc: 0.9973 - val_loss: 0.0300 - val_acc: 0.9927\n",
      "Epoch 6/12\n",
      " - 102s - loss: 0.0068 - acc: 0.9979 - val_loss: 0.0262 - val_acc: 0.9932\n",
      "Epoch 7/12\n",
      " - 102s - loss: 0.0058 - acc: 0.9980 - val_loss: 0.0327 - val_acc: 0.9924\n",
      "Epoch 8/12\n",
      " - 102s - loss: 0.0059 - acc: 0.9982 - val_loss: 0.0285 - val_acc: 0.9937\n",
      "Epoch 9/12\n",
      " - 102s - loss: 0.0048 - acc: 0.9986 - val_loss: 0.0301 - val_acc: 0.9936\n",
      "Epoch 10/12\n",
      " - 102s - loss: 0.0036 - acc: 0.9988 - val_loss: 0.0294 - val_acc: 0.9935\n",
      "Epoch 11/12\n",
      " - 102s - loss: 0.0055 - acc: 0.9983 - val_loss: 0.0300 - val_acc: 0.9937\n",
      "Epoch 12/12\n",
      " - 102s - loss: 0.0044 - acc: 0.9985 - val_loss: 0.0346 - val_acc: 0.9924\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb91ab3e358>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(batches, steps_per_epoch=int(math.ceil(batches.n / batches.batch_size)), epochs=12, verbose=2,\n",
    "                 validation_data=test_batches, validation_steps=int(math.ceil(test_batches.n/test_batches.batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Norm + Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_augmentaton = image.ImageDataGenerator(rotation_range=8, width_shift_range=0.08, height_shift_range=0.08, \n",
    "                         shear_range=0.3, zoom_range=0.08)\n",
    "\n",
    "batches = gen_augmentaton.flow(X_train, y_train, batch_size=batch_size)\n",
    "test_batches = gen.flow(X_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = get_model_bn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      " - 103s - loss: 0.1630 - acc: 0.9497 - val_loss: 0.0313 - val_acc: 0.9899\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb9134ecac8>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(batches, steps_per_epoch=int(math.ceil(batches.n / batches.batch_size)), epochs=1, verbose=2,\n",
    "                 validation_data=test_batches, validation_steps=int(math.ceil(test_batches.n/test_batches.batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      " - 102s - loss: 0.0475 - acc: 0.9851 - val_loss: 0.0232 - val_acc: 0.9931\n",
      "Epoch 2/3\n",
      " - 102s - loss: 0.0437 - acc: 0.9865 - val_loss: 0.0294 - val_acc: 0.9926\n",
      "Epoch 3/3\n",
      " - 102s - loss: 0.0410 - acc: 0.9872 - val_loss: 0.0263 - val_acc: 0.9906\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb912144a90>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(batches, steps_per_epoch=int(math.ceil(batches.n / batches.batch_size)), epochs=3, verbose=2,\n",
    "                 validation_data=test_batches, validation_steps=int(math.ceil(test_batches.n/test_batches.batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      " - 102s - loss: 0.0374 - acc: 0.9883 - val_loss: 0.0317 - val_acc: 0.9916\n",
      "Epoch 2/5\n",
      " - 102s - loss: 0.0356 - acc: 0.9893 - val_loss: 0.0175 - val_acc: 0.9945\n",
      "Epoch 3/5\n",
      " - 102s - loss: 0.0328 - acc: 0.9897 - val_loss: 0.0172 - val_acc: 0.9940\n",
      "Epoch 4/5\n",
      " - 102s - loss: 0.0329 - acc: 0.9897 - val_loss: 0.0165 - val_acc: 0.9941\n",
      "Epoch 5/5\n",
      " - 102s - loss: 0.0300 - acc: 0.9905 - val_loss: 0.0153 - val_acc: 0.9949\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb91212c4e0>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(batches, steps_per_epoch=int(math.ceil(batches.n / batches.batch_size)), epochs=5, verbose=2,\n",
    "                 validation_data=test_batches, validation_steps=int(math.ceil(test_batches.n/test_batches.batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      " - 102s - loss: 0.0296 - acc: 0.9906 - val_loss: 0.0215 - val_acc: 0.9941\n",
      "Epoch 2/12\n",
      " - 102s - loss: 0.0297 - acc: 0.9904 - val_loss: 0.0169 - val_acc: 0.9951\n",
      "Epoch 3/12\n",
      " - 102s - loss: 0.0270 - acc: 0.9915 - val_loss: 0.0186 - val_acc: 0.9945\n",
      "Epoch 4/12\n",
      " - 102s - loss: 0.0241 - acc: 0.9926 - val_loss: 0.0161 - val_acc: 0.9949\n",
      "Epoch 5/12\n",
      " - 102s - loss: 0.0248 - acc: 0.9921 - val_loss: 0.0168 - val_acc: 0.9953\n",
      "Epoch 6/12\n",
      " - 102s - loss: 0.0243 - acc: 0.9922 - val_loss: 0.0157 - val_acc: 0.9947\n",
      "Epoch 7/12\n",
      " - 102s - loss: 0.0250 - acc: 0.9924 - val_loss: 0.0187 - val_acc: 0.9943\n",
      "Epoch 8/12\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-136-cb6b2ee8c8e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model.fit_generator(batches, steps_per_epoch=int(math.ceil(batches.n / batches.batch_size)), epochs=12, verbose=2,\n\u001b[0;32m----> 2\u001b[0;31m                  validation_data=test_batches, validation_steps=int(math.ceil(test_batches.n/test_batches.batch_size)))\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1154\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m                                         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1156\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2075\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2076\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2077\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2078\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2079\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1795\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1796\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1797\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1798\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1799\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2330\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2331\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2332\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2333\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit_generator(batches, steps_per_epoch=int(math.ceil(batches.n / batches.batch_size)), epochs=12, verbose=2,\n",
    "                 validation_data=test_batches, validation_steps=int(math.ceil(test_batches.n/test_batches.batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Norm + Data Augmentation + Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_bn_dropout():\n",
    "    model = Sequential([\n",
    "        Lambda(norm_input, input_shape=(1, 28, 28)),\n",
    "        Conv2D(32, (3,3), activation='relu'),\n",
    "        BatchNormalization(axis=1),\n",
    "        Conv2D(32, (3,3), activation='relu'),\n",
    "        MaxPooling2D(),\n",
    "        BatchNormalization(axis=1),\n",
    "        Conv2D(64, (3,3), activation='relu'),\n",
    "        BatchNormalization(axis=1),\n",
    "        Conv2D(64, (3,3), activation='relu'),\n",
    "        MaxPooling2D(),\n",
    "        Flatten(),\n",
    "        BatchNormalization(),\n",
    "        Dense(512, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model_bn_dropout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "\n",
      "  1/938 [..............................] - ETA: 18:17 - loss: 3.7182 - acc: 0.1406\n",
      "  2/938 [..............................] - ETA: 9:57 - loss: 3.3286 - acc: 0.2031 \n",
      "  3/938 [..............................] - ETA: 7:11 - loss: 3.0250 - acc: 0.2396\n",
      "  4/938 [..............................] - ETA: 5:46 - loss: 2.7212 - acc: 0.3125\n",
      "  5/938 [..............................] - ETA: 4:57 - loss: 2.4608 - acc: 0.3719\n",
      "  6/938 [..............................] - ETA: 4:22 - loss: 2.3754 - acc: 0.3854\n",
      "  7/938 [..............................] - ETA: 3:58 - loss: 2.2578 - acc: 0.4040\n",
      "  8/938 [..............................] - ETA: 3:40 - loss: 2.1451 - acc: 0.4219\n",
      "  9/938 [..............................] - ETA: 3:26 - loss: 2.0587 - acc: 0.4479\n",
      " 10/938 [..............................] - ETA: 3:14 - loss: 1.9308 - acc: 0.4766\n",
      " 11/938 [..............................] - ETA: 3:05 - loss: 1.8610 - acc: 0.4901\n",
      " 12/938 [..............................] - ETA: 2:57 - loss: 1.8077 - acc: 0.5065\n",
      " 13/938 [..............................] - ETA: 2:51 - loss: 1.7356 - acc: 0.5240\n",
      " 14/938 [..............................] - ETA: 2:45 - loss: 1.7027 - acc: 0.5335\n",
      " 15/938 [..............................] - ETA: 2:41 - loss: 1.6513 - acc: 0.5437\n",
      " 16/938 [..............................] - ETA: 2:36 - loss: 1.6232 - acc: 0.5498\n",
      " 17/938 [..............................] - ETA: 2:32 - loss: 1.5916 - acc: 0.5625\n",
      " 18/938 [..............................] - ETA: 2:29 - loss: 1.5395 - acc: 0.5773\n",
      " 19/938 [..............................] - ETA: 2:26 - loss: 1.5072 - acc: 0.5880\n",
      " 20/938 [..............................] - ETA: 2:23 - loss: 1.4678 - acc: 0.5984\n",
      " 21/938 [..............................] - ETA: 2:21 - loss: 1.4247 - acc: 0.6094\n",
      " 22/938 [..............................] - ETA: 2:18 - loss: 1.3952 - acc: 0.6172\n",
      " 23/938 [..............................] - ETA: 2:16 - loss: 1.3774 - acc: 0.6230\n",
      " 24/938 [..............................] - ETA: 2:14 - loss: 1.3569 - acc: 0.6296\n",
      " 25/938 [..............................] - ETA: 2:12 - loss: 1.3321 - acc: 0.6356\n",
      " 26/938 [..............................] - ETA: 2:11 - loss: 1.2998 - acc: 0.6448\n",
      " 27/938 [..............................] - ETA: 2:09 - loss: 1.2750 - acc: 0.6522\n",
      " 28/938 [..............................] - ETA: 2:07 - loss: 1.2689 - acc: 0.6540\n",
      " 29/938 [..............................] - ETA: 2:06 - loss: 1.2471 - acc: 0.6606\n",
      " 30/938 [..............................] - ETA: 2:05 - loss: 1.2321 - acc: 0.6641\n",
      " 31/938 [..............................] - ETA: 2:03 - loss: 1.2176 - acc: 0.6668\n",
      " 32/938 [>.............................] - ETA: 2:02 - loss: 1.1876 - acc: 0.6743\n",
      " 33/938 [>.............................] - ETA: 2:01 - loss: 1.1654 - acc: 0.6795\n",
      " 34/938 [>.............................] - ETA: 2:00 - loss: 1.1473 - acc: 0.6834\n",
      " 35/938 [>.............................] - ETA: 1:58 - loss: 1.1270 - acc: 0.6871\n",
      " 36/938 [>.............................] - ETA: 1:57 - loss: 1.1136 - acc: 0.6910\n",
      " 37/938 [>.............................] - ETA: 1:56 - loss: 1.1036 - acc: 0.6934\n",
      " 38/938 [>.............................] - ETA: 1:56 - loss: 1.0884 - acc: 0.6970\n",
      " 39/938 [>.............................] - ETA: 1:55 - loss: 1.0714 - acc: 0.6999\n",
      " 40/938 [>.............................] - ETA: 1:54 - loss: 1.0633 - acc: 0.7023\n",
      " 41/938 [>.............................] - ETA: 1:54 - loss: 1.0554 - acc: 0.7046\n",
      " 42/938 [>.............................] - ETA: 1:53 - loss: 1.0376 - acc: 0.7094\n",
      " 43/938 [>.............................] - ETA: 1:52 - loss: 1.0242 - acc: 0.7129\n",
      " 44/938 [>.............................] - ETA: 1:52 - loss: 1.0173 - acc: 0.7148\n",
      " 45/938 [>.............................] - ETA: 1:51 - loss: 0.9992 - acc: 0.7194\n",
      " 46/938 [>.............................] - ETA: 1:50 - loss: 0.9879 - acc: 0.7228\n",
      " 47/938 [>.............................] - ETA: 1:50 - loss: 0.9815 - acc: 0.7254\n",
      " 48/938 [>.............................] - ETA: 1:49 - loss: 0.9659 - acc: 0.7295\n",
      " 49/938 [>.............................] - ETA: 1:49 - loss: 0.9548 - acc: 0.7331\n",
      " 50/938 [>.............................] - ETA: 1:48 - loss: 0.9449 - acc: 0.7356\n",
      " 51/938 [>.............................] - ETA: 1:48 - loss: 0.9372 - acc: 0.7368\n",
      " 52/938 [>.............................] - ETA: 1:47 - loss: 0.9272 - acc: 0.7395\n",
      " 53/938 [>.............................] - ETA: 1:47 - loss: 0.9192 - acc: 0.7417\n",
      " 54/938 [>.............................] - ETA: 1:47 - loss: 0.9073 - acc: 0.7442\n",
      " 55/938 [>.............................] - ETA: 1:46 - loss: 0.8959 - acc: 0.7477\n",
      " 56/938 [>.............................] - ETA: 1:46 - loss: 0.8908 - acc: 0.7492\n",
      " 57/938 [>.............................] - ETA: 1:45 - loss: 0.8852 - acc: 0.7497\n",
      " 58/938 [>.............................] - ETA: 1:45 - loss: 0.8751 - acc: 0.7527\n",
      " 59/938 [>.............................] - ETA: 1:44 - loss: 0.8637 - acc: 0.7558\n",
      " 60/938 [>.............................] - ETA: 1:44 - loss: 0.8567 - acc: 0.7581\n",
      " 61/938 [>.............................] - ETA: 1:44 - loss: 0.8520 - acc: 0.7595\n",
      " 62/938 [>.............................] - ETA: 1:43 - loss: 0.8522 - acc: 0.7601\n",
      " 63/938 [=>............................] - ETA: 1:43 - loss: 0.8439 - acc: 0.7626\n",
      " 64/938 [=>............................] - ETA: 1:43 - loss: 0.8350 - acc: 0.7646\n",
      " 65/938 [=>............................] - ETA: 1:42 - loss: 0.8280 - acc: 0.7666\n",
      " 66/938 [=>............................] - ETA: 1:42 - loss: 0.8215 - acc: 0.7687\n",
      " 67/938 [=>............................] - ETA: 1:42 - loss: 0.8151 - acc: 0.7708\n",
      " 68/938 [=>............................] - ETA: 1:42 - loss: 0.8087 - acc: 0.7725\n",
      " 69/938 [=>............................] - ETA: 1:41 - loss: 0.8008 - acc: 0.7736\n",
      " 70/938 [=>............................] - ETA: 1:41 - loss: 0.7919 - acc: 0.7759\n",
      " 71/938 [=>............................] - ETA: 1:41 - loss: 0.7862 - acc: 0.7766\n",
      " 72/938 [=>............................] - ETA: 1:40 - loss: 0.7802 - acc: 0.7782\n",
      " 73/938 [=>............................] - ETA: 1:40 - loss: 0.7787 - acc: 0.7791\n",
      " 74/938 [=>............................] - ETA: 1:40 - loss: 0.7777 - acc: 0.7802\n",
      " 75/938 [=>............................] - ETA: 1:40 - loss: 0.7771 - acc: 0.7808\n",
      " 76/938 [=>............................] - ETA: 1:39 - loss: 0.7706 - acc: 0.7825\n",
      " 77/938 [=>............................] - ETA: 1:39 - loss: 0.7650 - acc: 0.7833\n",
      " 78/938 [=>............................] - ETA: 1:39 - loss: 0.7581 - acc: 0.7853\n",
      " 79/938 [=>............................] - ETA: 1:39 - loss: 0.7555 - acc: 0.7860\n",
      " 80/938 [=>............................] - ETA: 1:38 - loss: 0.7515 - acc: 0.7869\n",
      " 81/938 [=>............................] - ETA: 1:38 - loss: 0.7485 - acc: 0.7878\n",
      " 82/938 [=>............................] - ETA: 1:38 - loss: 0.7429 - acc: 0.7887\n",
      " 83/938 [=>............................] - ETA: 1:38 - loss: 0.7369 - acc: 0.7899\n",
      " 84/938 [=>............................] - ETA: 1:37 - loss: 0.7331 - acc: 0.7913\n",
      " 85/938 [=>............................] - ETA: 1:37 - loss: 0.7272 - acc: 0.7930\n",
      " 86/938 [=>............................] - ETA: 1:37 - loss: 0.7223 - acc: 0.7936\n",
      " 87/938 [=>............................] - ETA: 1:37 - loss: 0.7185 - acc: 0.7947\n",
      " 88/938 [=>............................] - ETA: 1:36 - loss: 0.7171 - acc: 0.7953\n",
      " 89/938 [=>............................] - ETA: 1:36 - loss: 0.7143 - acc: 0.7963\n",
      " 90/938 [=>............................] - ETA: 1:36 - loss: 0.7096 - acc: 0.7972\n",
      " 91/938 [=>............................] - ETA: 1:36 - loss: 0.7070 - acc: 0.7981\n",
      " 92/938 [=>............................] - ETA: 1:36 - loss: 0.7040 - acc: 0.7984\n",
      " 93/938 [=>............................] - ETA: 1:35 - loss: 0.7005 - acc: 0.7994\n",
      " 94/938 [==>...........................] - ETA: 1:35 - loss: 0.6971 - acc: 0.8002\n",
      " 95/938 [==>...........................] - ETA: 1:35 - loss: 0.6934 - acc: 0.8013\n",
      " 96/938 [==>...........................] - ETA: 1:35 - loss: 0.6892 - acc: 0.8024\n",
      " 97/938 [==>...........................] - ETA: 1:35 - loss: 0.6867 - acc: 0.8033\n",
      " 98/938 [==>...........................] - ETA: 1:34 - loss: 0.6837 - acc: 0.8040\n",
      " 99/938 [==>...........................] - ETA: 1:34 - loss: 0.6797 - acc: 0.8048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/938 [==>...........................] - ETA: 1:34 - loss: 0.6769 - acc: 0.8058\n",
      "101/938 [==>...........................] - ETA: 1:34 - loss: 0.6732 - acc: 0.8068\n",
      "102/938 [==>...........................] - ETA: 1:33 - loss: 0.6710 - acc: 0.8068\n",
      "103/938 [==>...........................] - ETA: 1:33 - loss: 0.6674 - acc: 0.8075\n",
      "104/938 [==>...........................] - ETA: 1:33 - loss: 0.6658 - acc: 0.8077\n",
      "105/938 [==>...........................] - ETA: 1:33 - loss: 0.6607 - acc: 0.8091\n",
      "106/938 [==>...........................] - ETA: 1:33 - loss: 0.6598 - acc: 0.8096\n",
      "107/938 [==>...........................] - ETA: 1:33 - loss: 0.6562 - acc: 0.8109\n",
      "108/938 [==>...........................] - ETA: 1:32 - loss: 0.6537 - acc: 0.8115\n",
      "109/938 [==>...........................] - ETA: 1:32 - loss: 0.6493 - acc: 0.8126\n",
      "110/938 [==>...........................] - ETA: 1:32 - loss: 0.6456 - acc: 0.8135\n",
      "111/938 [==>...........................] - ETA: 1:32 - loss: 0.6416 - acc: 0.8146\n",
      "112/938 [==>...........................] - ETA: 1:32 - loss: 0.6388 - acc: 0.8152\n",
      "113/938 [==>...........................] - ETA: 1:31 - loss: 0.6360 - acc: 0.8154\n",
      "114/938 [==>...........................] - ETA: 1:31 - loss: 0.6323 - acc: 0.8165\n",
      "115/938 [==>...........................] - ETA: 1:31 - loss: 0.6305 - acc: 0.8164\n",
      "116/938 [==>...........................] - ETA: 1:31 - loss: 0.6267 - acc: 0.8175\n",
      "117/938 [==>...........................] - ETA: 1:31 - loss: 0.6236 - acc: 0.8184\n",
      "118/938 [==>...........................] - ETA: 1:31 - loss: 0.6228 - acc: 0.8186\n",
      "119/938 [==>...........................] - ETA: 1:30 - loss: 0.6215 - acc: 0.8193\n",
      "120/938 [==>...........................] - ETA: 1:30 - loss: 0.6182 - acc: 0.8202\n",
      "121/938 [==>...........................] - ETA: 1:30 - loss: 0.6155 - acc: 0.8210\n",
      "122/938 [==>...........................] - ETA: 1:30 - loss: 0.6139 - acc: 0.8216\n",
      "123/938 [==>...........................] - ETA: 1:30 - loss: 0.6108 - acc: 0.8224\n",
      "124/938 [==>...........................] - ETA: 1:30 - loss: 0.6082 - acc: 0.8230\n",
      "125/938 [==>...........................] - ETA: 1:29 - loss: 0.6065 - acc: 0.8235\n",
      "126/938 [===>..........................] - ETA: 1:29 - loss: 0.6032 - acc: 0.8245\n",
      "127/938 [===>..........................] - ETA: 1:29 - loss: 0.6007 - acc: 0.8249\n",
      "128/938 [===>..........................] - ETA: 1:29 - loss: 0.5971 - acc: 0.8258\n",
      "129/938 [===>..........................] - ETA: 1:29 - loss: 0.5953 - acc: 0.8264\n",
      "130/938 [===>..........................] - ETA: 1:29 - loss: 0.5921 - acc: 0.8272\n",
      "131/938 [===>..........................] - ETA: 1:28 - loss: 0.5903 - acc: 0.8278\n",
      "132/938 [===>..........................] - ETA: 1:28 - loss: 0.5879 - acc: 0.8282\n",
      "133/938 [===>..........................] - ETA: 1:28 - loss: 0.5849 - acc: 0.8289\n",
      "134/938 [===>..........................] - ETA: 1:28 - loss: 0.5824 - acc: 0.8295\n",
      "135/938 [===>..........................] - ETA: 1:28 - loss: 0.5825 - acc: 0.8296\n",
      "136/938 [===>..........................] - ETA: 1:28 - loss: 0.5815 - acc: 0.8301\n",
      "137/938 [===>..........................] - ETA: 1:27 - loss: 0.5789 - acc: 0.8310\n",
      "138/938 [===>..........................] - ETA: 1:27 - loss: 0.5763 - acc: 0.8319\n",
      "139/938 [===>..........................] - ETA: 1:27 - loss: 0.5727 - acc: 0.8330\n",
      "140/938 [===>..........................] - ETA: 1:27 - loss: 0.5702 - acc: 0.8336\n",
      "141/938 [===>..........................] - ETA: 1:27 - loss: 0.5686 - acc: 0.8339\n",
      "142/938 [===>..........................] - ETA: 1:27 - loss: 0.5663 - acc: 0.8346\n",
      "143/938 [===>..........................] - ETA: 1:27 - loss: 0.5644 - acc: 0.8351\n",
      "144/938 [===>..........................] - ETA: 1:26 - loss: 0.5626 - acc: 0.8356\n",
      "145/938 [===>..........................] - ETA: 1:26 - loss: 0.5603 - acc: 0.8361\n",
      "146/938 [===>..........................] - ETA: 1:26 - loss: 0.5587 - acc: 0.8366\n",
      "147/938 [===>..........................] - ETA: 1:26 - loss: 0.5570 - acc: 0.8367\n",
      "148/938 [===>..........................] - ETA: 1:26 - loss: 0.5541 - acc: 0.8374\n",
      "149/938 [===>..........................] - ETA: 1:26 - loss: 0.5522 - acc: 0.8380\n",
      "150/938 [===>..........................] - ETA: 1:25 - loss: 0.5507 - acc: 0.8381\n",
      "151/938 [===>..........................] - ETA: 1:25 - loss: 0.5496 - acc: 0.8383\n",
      "152/938 [===>..........................] - ETA: 1:25 - loss: 0.5479 - acc: 0.8388\n",
      "153/938 [===>..........................] - ETA: 1:25 - loss: 0.5475 - acc: 0.8390\n",
      "154/938 [===>..........................] - ETA: 1:25 - loss: 0.5456 - acc: 0.8396\n",
      "155/938 [===>..........................] - ETA: 1:25 - loss: 0.5436 - acc: 0.8402\n",
      "156/938 [===>..........................] - ETA: 1:24 - loss: 0.5434 - acc: 0.8406\n",
      "157/938 [====>.........................] - ETA: 1:24 - loss: 0.5429 - acc: 0.8410\n",
      "158/938 [====>.........................] - ETA: 1:24 - loss: 0.5406 - acc: 0.8415\n",
      "159/938 [====>.........................] - ETA: 1:24 - loss: 0.5383 - acc: 0.8422\n",
      "160/938 [====>.........................] - ETA: 1:24 - loss: 0.5353 - acc: 0.8431\n",
      "161/938 [====>.........................] - ETA: 1:24 - loss: 0.5329 - acc: 0.8437\n",
      "162/938 [====>.........................] - ETA: 1:24 - loss: 0.5323 - acc: 0.8438\n",
      "163/938 [====>.........................] - ETA: 1:24 - loss: 0.5313 - acc: 0.8442\n",
      "164/938 [====>.........................] - ETA: 1:23 - loss: 0.5290 - acc: 0.8445\n",
      "165/938 [====>.........................] - ETA: 1:23 - loss: 0.5267 - acc: 0.8452\n",
      "166/938 [====>.........................] - ETA: 1:23 - loss: 0.5247 - acc: 0.8455\n",
      "167/938 [====>.........................] - ETA: 1:23 - loss: 0.5229 - acc: 0.8460\n",
      "168/938 [====>.........................] - ETA: 1:23 - loss: 0.5207 - acc: 0.8464\n",
      "169/938 [====>.........................] - ETA: 1:23 - loss: 0.5192 - acc: 0.8469\n",
      "170/938 [====>.........................] - ETA: 1:23 - loss: 0.5166 - acc: 0.8477\n",
      "171/938 [====>.........................] - ETA: 1:22 - loss: 0.5145 - acc: 0.8482\n",
      "172/938 [====>.........................] - ETA: 1:22 - loss: 0.5120 - acc: 0.8489\n",
      "173/938 [====>.........................] - ETA: 1:22 - loss: 0.5109 - acc: 0.8493\n",
      "174/938 [====>.........................] - ETA: 1:22 - loss: 0.5089 - acc: 0.8497\n",
      "175/938 [====>.........................] - ETA: 1:22 - loss: 0.5070 - acc: 0.8500\n",
      "176/938 [====>.........................] - ETA: 1:22 - loss: 0.5047 - acc: 0.8507\n",
      "177/938 [====>.........................] - ETA: 1:22 - loss: 0.5026 - acc: 0.8512\n",
      "178/938 [====>.........................] - ETA: 1:21 - loss: 0.5015 - acc: 0.8514\n",
      "179/938 [====>.........................] - ETA: 1:21 - loss: 0.5005 - acc: 0.8519\n",
      "180/938 [====>.........................] - ETA: 1:21 - loss: 0.4988 - acc: 0.8521\n",
      "181/938 [====>.........................] - ETA: 1:21 - loss: 0.4968 - acc: 0.8526\n",
      "182/938 [====>.........................] - ETA: 1:21 - loss: 0.4956 - acc: 0.8529\n",
      "183/938 [====>.........................] - ETA: 1:21 - loss: 0.4940 - acc: 0.8532\n",
      "184/938 [====>.........................] - ETA: 1:21 - loss: 0.4919 - acc: 0.8539\n",
      "185/938 [====>.........................] - ETA: 1:21 - loss: 0.4906 - acc: 0.8542\n",
      "186/938 [====>.........................] - ETA: 1:20 - loss: 0.4886 - acc: 0.8549\n",
      "187/938 [====>.........................] - ETA: 1:20 - loss: 0.4867 - acc: 0.8554\n",
      "188/938 [=====>........................] - ETA: 1:20 - loss: 0.4859 - acc: 0.8556\n",
      "189/938 [=====>........................] - ETA: 1:20 - loss: 0.4846 - acc: 0.8559\n",
      "190/938 [=====>........................] - ETA: 1:20 - loss: 0.4837 - acc: 0.8562\n",
      "191/938 [=====>........................] - ETA: 1:20 - loss: 0.4835 - acc: 0.8564\n",
      "192/938 [=====>........................] - ETA: 1:20 - loss: 0.4813 - acc: 0.8570\n",
      "193/938 [=====>........................] - ETA: 1:19 - loss: 0.4807 - acc: 0.8573\n",
      "194/938 [=====>........................] - ETA: 1:19 - loss: 0.4803 - acc: 0.8574\n",
      "195/938 [=====>........................] - ETA: 1:19 - loss: 0.4800 - acc: 0.8575\n",
      "196/938 [=====>........................] - ETA: 1:19 - loss: 0.4784 - acc: 0.8580\n",
      "197/938 [=====>........................] - ETA: 1:19 - loss: 0.4763 - acc: 0.8587\n",
      "198/938 [=====>........................] - ETA: 1:19 - loss: 0.4751 - acc: 0.8591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199/938 [=====>........................] - ETA: 1:19 - loss: 0.4737 - acc: 0.8595\n",
      "200/938 [=====>........................] - ETA: 1:19 - loss: 0.4732 - acc: 0.8598\n",
      "201/938 [=====>........................] - ETA: 1:18 - loss: 0.4716 - acc: 0.8602\n",
      "202/938 [=====>........................] - ETA: 1:18 - loss: 0.4699 - acc: 0.8608\n",
      "203/938 [=====>........................] - ETA: 1:18 - loss: 0.4685 - acc: 0.8611\n",
      "204/938 [=====>........................] - ETA: 1:18 - loss: 0.4673 - acc: 0.8615\n",
      "205/938 [=====>........................] - ETA: 1:18 - loss: 0.4662 - acc: 0.8619\n",
      "206/938 [=====>........................] - ETA: 1:18 - loss: 0.4653 - acc: 0.8620\n",
      "207/938 [=====>........................] - ETA: 1:18 - loss: 0.4643 - acc: 0.8622\n",
      "208/938 [=====>........................] - ETA: 1:18 - loss: 0.4631 - acc: 0.8626\n",
      "209/938 [=====>........................] - ETA: 1:17 - loss: 0.4621 - acc: 0.8628\n",
      "210/938 [=====>........................] - ETA: 1:17 - loss: 0.4606 - acc: 0.8632\n",
      "211/938 [=====>........................] - ETA: 1:17 - loss: 0.4593 - acc: 0.8634\n",
      "212/938 [=====>........................] - ETA: 1:17 - loss: 0.4585 - acc: 0.8636\n",
      "213/938 [=====>........................] - ETA: 1:17 - loss: 0.4571 - acc: 0.8641\n",
      "214/938 [=====>........................] - ETA: 1:17 - loss: 0.4563 - acc: 0.8642\n",
      "215/938 [=====>........................] - ETA: 1:17 - loss: 0.4556 - acc: 0.8644\n",
      "216/938 [=====>........................] - ETA: 1:17 - loss: 0.4544 - acc: 0.8648\n",
      "217/938 [=====>........................] - ETA: 1:17 - loss: 0.4545 - acc: 0.8648\n",
      "218/938 [=====>........................] - ETA: 1:16 - loss: 0.4533 - acc: 0.8652\n",
      "219/938 [======>.......................] - ETA: 1:16 - loss: 0.4514 - acc: 0.8658\n",
      "220/938 [======>.......................] - ETA: 1:16 - loss: 0.4501 - acc: 0.8662\n",
      "221/938 [======>.......................] - ETA: 1:16 - loss: 0.4498 - acc: 0.8660\n",
      "222/938 [======>.......................] - ETA: 1:16 - loss: 0.4490 - acc: 0.8661\n",
      "223/938 [======>.......................] - ETA: 1:16 - loss: 0.4481 - acc: 0.8664\n",
      "224/938 [======>.......................] - ETA: 1:16 - loss: 0.4464 - acc: 0.8668\n",
      "225/938 [======>.......................] - ETA: 1:16 - loss: 0.4448 - acc: 0.8673\n",
      "226/938 [======>.......................] - ETA: 1:15 - loss: 0.4435 - acc: 0.8677\n",
      "227/938 [======>.......................] - ETA: 1:15 - loss: 0.4429 - acc: 0.8680\n",
      "228/938 [======>.......................] - ETA: 1:15 - loss: 0.4417 - acc: 0.8683\n",
      "229/938 [======>.......................] - ETA: 1:15 - loss: 0.4399 - acc: 0.8689\n",
      "230/938 [======>.......................] - ETA: 1:15 - loss: 0.4405 - acc: 0.8687\n",
      "231/938 [======>.......................] - ETA: 1:15 - loss: 0.4397 - acc: 0.8689\n",
      "232/938 [======>.......................] - ETA: 1:15 - loss: 0.4385 - acc: 0.8693\n",
      "233/938 [======>.......................] - ETA: 1:15 - loss: 0.4375 - acc: 0.8696\n",
      "234/938 [======>.......................] - ETA: 1:15 - loss: 0.4363 - acc: 0.8699\n",
      "235/938 [======>.......................] - ETA: 1:14 - loss: 0.4353 - acc: 0.8702\n",
      "236/938 [======>.......................] - ETA: 1:14 - loss: 0.4337 - acc: 0.8706\n",
      "237/938 [======>.......................] - ETA: 1:14 - loss: 0.4332 - acc: 0.8708\n",
      "238/938 [======>.......................] - ETA: 1:14 - loss: 0.4321 - acc: 0.8712\n",
      "239/938 [======>.......................] - ETA: 1:14 - loss: 0.4309 - acc: 0.8715\n",
      "240/938 [======>.......................] - ETA: 1:14 - loss: 0.4300 - acc: 0.8717\n",
      "241/938 [======>.......................] - ETA: 1:14 - loss: 0.4289 - acc: 0.8720\n",
      "242/938 [======>.......................] - ETA: 1:14 - loss: 0.4285 - acc: 0.8722\n",
      "243/938 [======>.......................] - ETA: 1:13 - loss: 0.4277 - acc: 0.8725\n",
      "244/938 [======>.......................] - ETA: 1:13 - loss: 0.4265 - acc: 0.8729\n",
      "245/938 [======>.......................] - ETA: 1:13 - loss: 0.4252 - acc: 0.8733\n",
      "246/938 [======>.......................] - ETA: 1:13 - loss: 0.4240 - acc: 0.8736\n",
      "247/938 [======>.......................] - ETA: 1:13 - loss: 0.4231 - acc: 0.8738\n",
      "248/938 [======>.......................] - ETA: 1:13 - loss: 0.4225 - acc: 0.8741\n",
      "249/938 [======>.......................] - ETA: 1:13 - loss: 0.4216 - acc: 0.8743\n",
      "250/938 [======>.......................] - ETA: 1:13 - loss: 0.4209 - acc: 0.8745\n",
      "251/938 [=======>......................] - ETA: 1:12 - loss: 0.4211 - acc: 0.8745\n",
      "252/938 [=======>......................] - ETA: 1:12 - loss: 0.4202 - acc: 0.8748\n",
      "253/938 [=======>......................] - ETA: 1:12 - loss: 0.4196 - acc: 0.8750\n",
      "254/938 [=======>......................] - ETA: 1:12 - loss: 0.4193 - acc: 0.8749\n",
      "255/938 [=======>......................] - ETA: 1:12 - loss: 0.4179 - acc: 0.8753\n",
      "256/938 [=======>......................] - ETA: 1:12 - loss: 0.4168 - acc: 0.8757\n",
      "257/938 [=======>......................] - ETA: 1:12 - loss: 0.4170 - acc: 0.8758\n",
      "258/938 [=======>......................] - ETA: 1:12 - loss: 0.4164 - acc: 0.8758\n",
      "259/938 [=======>......................] - ETA: 1:11 - loss: 0.4153 - acc: 0.8762\n",
      "260/938 [=======>......................] - ETA: 1:11 - loss: 0.4144 - acc: 0.8764\n",
      "261/938 [=======>......................] - ETA: 1:11 - loss: 0.4139 - acc: 0.8765\n",
      "262/938 [=======>......................] - ETA: 1:11 - loss: 0.4131 - acc: 0.8767\n",
      "263/938 [=======>......................] - ETA: 1:11 - loss: 0.4124 - acc: 0.8768\n",
      "264/938 [=======>......................] - ETA: 1:11 - loss: 0.4111 - acc: 0.8772\n",
      "265/938 [=======>......................] - ETA: 1:11 - loss: 0.4104 - acc: 0.8774\n",
      "266/938 [=======>......................] - ETA: 1:11 - loss: 0.4096 - acc: 0.8775\n",
      "267/938 [=======>......................] - ETA: 1:11 - loss: 0.4090 - acc: 0.8778\n",
      "268/938 [=======>......................] - ETA: 1:10 - loss: 0.4087 - acc: 0.8780\n",
      "269/938 [=======>......................] - ETA: 1:10 - loss: 0.4079 - acc: 0.8782\n",
      "270/938 [=======>......................] - ETA: 1:10 - loss: 0.4072 - acc: 0.8784\n",
      "271/938 [=======>......................] - ETA: 1:10 - loss: 0.4066 - acc: 0.8786\n",
      "272/938 [=======>......................] - ETA: 1:10 - loss: 0.4057 - acc: 0.8787\n",
      "273/938 [=======>......................] - ETA: 1:10 - loss: 0.4053 - acc: 0.8789\n",
      "274/938 [=======>......................] - ETA: 1:10 - loss: 0.4046 - acc: 0.8792\n",
      "275/938 [=======>......................] - ETA: 1:10 - loss: 0.4038 - acc: 0.8794\n",
      "276/938 [=======>......................] - ETA: 1:09 - loss: 0.4031 - acc: 0.8796\n",
      "277/938 [=======>......................] - ETA: 1:09 - loss: 0.4023 - acc: 0.8799\n",
      "278/938 [=======>......................] - ETA: 1:09 - loss: 0.4014 - acc: 0.8801\n",
      "279/938 [=======>......................] - ETA: 1:09 - loss: 0.4008 - acc: 0.8803\n",
      "280/938 [=======>......................] - ETA: 1:09 - loss: 0.3999 - acc: 0.8805\n",
      "281/938 [=======>......................] - ETA: 1:09 - loss: 0.3990 - acc: 0.8808\n",
      "282/938 [========>.....................] - ETA: 1:09 - loss: 0.3980 - acc: 0.8810\n",
      "283/938 [========>.....................] - ETA: 1:09 - loss: 0.3971 - acc: 0.8813\n",
      "284/938 [========>.....................] - ETA: 1:09 - loss: 0.3962 - acc: 0.8816\n",
      "285/938 [========>.....................] - ETA: 1:08 - loss: 0.3954 - acc: 0.8819\n",
      "286/938 [========>.....................] - ETA: 1:08 - loss: 0.3950 - acc: 0.8819\n",
      "287/938 [========>.....................] - ETA: 1:08 - loss: 0.3949 - acc: 0.8820\n",
      "288/938 [========>.....................] - ETA: 1:08 - loss: 0.3942 - acc: 0.8821\n",
      "289/938 [========>.....................] - ETA: 1:08 - loss: 0.3936 - acc: 0.8822\n",
      "290/938 [========>.....................] - ETA: 1:08 - loss: 0.3926 - acc: 0.8825\n",
      "291/938 [========>.....................] - ETA: 1:08 - loss: 0.3916 - acc: 0.8827\n",
      "292/938 [========>.....................] - ETA: 1:08 - loss: 0.3908 - acc: 0.8830\n",
      "293/938 [========>.....................] - ETA: 1:08 - loss: 0.3898 - acc: 0.8833\n",
      "294/938 [========>.....................] - ETA: 1:07 - loss: 0.3888 - acc: 0.8835\n",
      "295/938 [========>.....................] - ETA: 1:07 - loss: 0.3878 - acc: 0.8838\n",
      "296/938 [========>.....................] - ETA: 1:07 - loss: 0.3871 - acc: 0.8840\n",
      "297/938 [========>.....................] - ETA: 1:07 - loss: 0.3860 - acc: 0.8843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "298/938 [========>.....................] - ETA: 1:07 - loss: 0.3849 - acc: 0.8847\n",
      "299/938 [========>.....................] - ETA: 1:07 - loss: 0.3846 - acc: 0.8848\n",
      "300/938 [========>.....................] - ETA: 1:07 - loss: 0.3837 - acc: 0.8851\n",
      "301/938 [========>.....................] - ETA: 1:07 - loss: 0.3829 - acc: 0.8852\n",
      "302/938 [========>.....................] - ETA: 1:07 - loss: 0.3823 - acc: 0.8853\n",
      "303/938 [========>.....................] - ETA: 1:06 - loss: 0.3820 - acc: 0.8854\n",
      "304/938 [========>.....................] - ETA: 1:06 - loss: 0.3810 - acc: 0.8856\n",
      "305/938 [========>.....................] - ETA: 1:06 - loss: 0.3800 - acc: 0.8859\n",
      "306/938 [========>.....................] - ETA: 1:06 - loss: 0.3791 - acc: 0.8862\n",
      "307/938 [========>.....................] - ETA: 1:06 - loss: 0.3786 - acc: 0.8863\n",
      "308/938 [========>.....................] - ETA: 1:06 - loss: 0.3781 - acc: 0.8865\n",
      "309/938 [========>.....................] - ETA: 1:06 - loss: 0.3776 - acc: 0.8865\n",
      "310/938 [========>.....................] - ETA: 1:06 - loss: 0.3769 - acc: 0.8867\n",
      "311/938 [========>.....................] - ETA: 1:06 - loss: 0.3760 - acc: 0.8870\n",
      "312/938 [========>.....................] - ETA: 1:05 - loss: 0.3752 - acc: 0.8872\n",
      "313/938 [=========>....................] - ETA: 1:05 - loss: 0.3747 - acc: 0.8872\n",
      "314/938 [=========>....................] - ETA: 1:05 - loss: 0.3745 - acc: 0.8874\n",
      "315/938 [=========>....................] - ETA: 1:05 - loss: 0.3740 - acc: 0.8876\n",
      "316/938 [=========>....................] - ETA: 1:05 - loss: 0.3735 - acc: 0.8878\n",
      "317/938 [=========>....................] - ETA: 1:05 - loss: 0.3727 - acc: 0.8881\n",
      "318/938 [=========>....................] - ETA: 1:05 - loss: 0.3722 - acc: 0.8883\n",
      "319/938 [=========>....................] - ETA: 1:05 - loss: 0.3718 - acc: 0.8884\n",
      "320/938 [=========>....................] - ETA: 1:05 - loss: 0.3709 - acc: 0.8887\n",
      "321/938 [=========>....................] - ETA: 1:04 - loss: 0.3703 - acc: 0.8889\n",
      "322/938 [=========>....................] - ETA: 1:04 - loss: 0.3700 - acc: 0.8890\n",
      "323/938 [=========>....................] - ETA: 1:04 - loss: 0.3691 - acc: 0.8893\n",
      "324/938 [=========>....................] - ETA: 1:04 - loss: 0.3685 - acc: 0.8896\n",
      "325/938 [=========>....................] - ETA: 1:04 - loss: 0.3681 - acc: 0.8897\n",
      "326/938 [=========>....................] - ETA: 1:04 - loss: 0.3685 - acc: 0.8897\n",
      "327/938 [=========>....................] - ETA: 1:04 - loss: 0.3683 - acc: 0.8897\n",
      "328/938 [=========>....................] - ETA: 1:04 - loss: 0.3674 - acc: 0.8900\n",
      "329/938 [=========>....................] - ETA: 1:04 - loss: 0.3667 - acc: 0.8902\n",
      "330/938 [=========>....................] - ETA: 1:03 - loss: 0.3657 - acc: 0.8905\n",
      "331/938 [=========>....................] - ETA: 1:03 - loss: 0.3652 - acc: 0.8906\n",
      "332/938 [=========>....................] - ETA: 1:03 - loss: 0.3644 - acc: 0.8908\n",
      "333/938 [=========>....................] - ETA: 1:03 - loss: 0.3634 - acc: 0.8911\n",
      "334/938 [=========>....................] - ETA: 1:03 - loss: 0.3625 - acc: 0.8915\n",
      "335/938 [=========>....................] - ETA: 1:03 - loss: 0.3619 - acc: 0.8917\n",
      "336/938 [=========>....................] - ETA: 1:03 - loss: 0.3615 - acc: 0.8918\n",
      "337/938 [=========>....................] - ETA: 1:03 - loss: 0.3612 - acc: 0.8918\n",
      "338/938 [=========>....................] - ETA: 1:03 - loss: 0.3608 - acc: 0.8919\n",
      "339/938 [=========>....................] - ETA: 1:02 - loss: 0.3601 - acc: 0.8921\n",
      "340/938 [=========>....................] - ETA: 1:02 - loss: 0.3599 - acc: 0.8923\n",
      "341/938 [=========>....................] - ETA: 1:02 - loss: 0.3593 - acc: 0.8925\n",
      "342/938 [=========>....................] - ETA: 1:02 - loss: 0.3588 - acc: 0.8926\n",
      "343/938 [=========>....................] - ETA: 1:02 - loss: 0.3578 - acc: 0.8929\n",
      "344/938 [==========>...................] - ETA: 1:02 - loss: 0.3571 - acc: 0.8931\n",
      "345/938 [==========>...................] - ETA: 1:02 - loss: 0.3568 - acc: 0.8932\n",
      "346/938 [==========>...................] - ETA: 1:02 - loss: 0.3567 - acc: 0.8932\n",
      "347/938 [==========>...................] - ETA: 1:02 - loss: 0.3560 - acc: 0.8935\n",
      "348/938 [==========>...................] - ETA: 1:01 - loss: 0.3557 - acc: 0.8936\n",
      "349/938 [==========>...................] - ETA: 1:01 - loss: 0.3549 - acc: 0.8938\n",
      "350/938 [==========>...................] - ETA: 1:01 - loss: 0.3541 - acc: 0.8940\n",
      "351/938 [==========>...................] - ETA: 1:01 - loss: 0.3534 - acc: 0.8942\n",
      "352/938 [==========>...................] - ETA: 1:01 - loss: 0.3529 - acc: 0.8944\n",
      "353/938 [==========>...................] - ETA: 1:01 - loss: 0.3527 - acc: 0.8945\n",
      "354/938 [==========>...................] - ETA: 1:01 - loss: 0.3519 - acc: 0.8948\n",
      "355/938 [==========>...................] - ETA: 1:01 - loss: 0.3511 - acc: 0.8950\n",
      "356/938 [==========>...................] - ETA: 1:01 - loss: 0.3503 - acc: 0.8953\n",
      "357/938 [==========>...................] - ETA: 1:00 - loss: 0.3501 - acc: 0.8954\n",
      "358/938 [==========>...................] - ETA: 1:00 - loss: 0.3493 - acc: 0.8956\n",
      "359/938 [==========>...................] - ETA: 1:00 - loss: 0.3489 - acc: 0.8957\n",
      "360/938 [==========>...................] - ETA: 1:00 - loss: 0.3484 - acc: 0.8959\n",
      "361/938 [==========>...................] - ETA: 1:00 - loss: 0.3481 - acc: 0.8958\n",
      "362/938 [==========>...................] - ETA: 1:00 - loss: 0.3472 - acc: 0.8961\n",
      "363/938 [==========>...................] - ETA: 1:00 - loss: 0.3474 - acc: 0.8961\n",
      "364/938 [==========>...................] - ETA: 1:00 - loss: 0.3468 - acc: 0.8963\n",
      "365/938 [==========>...................] - ETA: 1:00 - loss: 0.3464 - acc: 0.8965\n",
      "366/938 [==========>...................] - ETA: 1:00 - loss: 0.3459 - acc: 0.8967\n",
      "367/938 [==========>...................] - ETA: 59s - loss: 0.3457 - acc: 0.8968 \n",
      "368/938 [==========>...................] - ETA: 59s - loss: 0.3452 - acc: 0.8970\n",
      "369/938 [==========>...................] - ETA: 59s - loss: 0.3443 - acc: 0.8972\n",
      "370/938 [==========>...................] - ETA: 59s - loss: 0.3438 - acc: 0.8974\n",
      "371/938 [==========>...................] - ETA: 59s - loss: 0.3434 - acc: 0.8975\n",
      "372/938 [==========>...................] - ETA: 59s - loss: 0.3427 - acc: 0.8978\n",
      "373/938 [==========>...................] - ETA: 59s - loss: 0.3421 - acc: 0.8979\n",
      "374/938 [==========>...................] - ETA: 59s - loss: 0.3419 - acc: 0.8981\n",
      "375/938 [==========>...................] - ETA: 59s - loss: 0.3414 - acc: 0.8982\n",
      "376/938 [===========>..................] - ETA: 58s - loss: 0.3406 - acc: 0.8985\n",
      "377/938 [===========>..................] - ETA: 58s - loss: 0.3398 - acc: 0.8987\n",
      "378/938 [===========>..................] - ETA: 58s - loss: 0.3391 - acc: 0.8990\n",
      "379/938 [===========>..................] - ETA: 58s - loss: 0.3385 - acc: 0.8992\n",
      "380/938 [===========>..................] - ETA: 58s - loss: 0.3379 - acc: 0.8993\n",
      "381/938 [===========>..................] - ETA: 58s - loss: 0.3375 - acc: 0.8994\n",
      "382/938 [===========>..................] - ETA: 58s - loss: 0.3369 - acc: 0.8995\n",
      "383/938 [===========>..................] - ETA: 58s - loss: 0.3365 - acc: 0.8997\n",
      "384/938 [===========>..................] - ETA: 58s - loss: 0.3365 - acc: 0.8997\n",
      "385/938 [===========>..................] - ETA: 57s - loss: 0.3363 - acc: 0.8999\n",
      "386/938 [===========>..................] - ETA: 57s - loss: 0.3357 - acc: 0.9000\n",
      "387/938 [===========>..................] - ETA: 57s - loss: 0.3351 - acc: 0.9002\n",
      "388/938 [===========>..................] - ETA: 57s - loss: 0.3347 - acc: 0.9003\n",
      "389/938 [===========>..................] - ETA: 57s - loss: 0.3343 - acc: 0.9004\n",
      "390/938 [===========>..................] - ETA: 57s - loss: 0.3339 - acc: 0.9005\n",
      "391/938 [===========>..................] - ETA: 57s - loss: 0.3335 - acc: 0.9007\n",
      "392/938 [===========>..................] - ETA: 57s - loss: 0.3333 - acc: 0.9008\n",
      "393/938 [===========>..................] - ETA: 57s - loss: 0.3329 - acc: 0.9009\n",
      "394/938 [===========>..................] - ETA: 56s - loss: 0.3323 - acc: 0.9011\n",
      "395/938 [===========>..................] - ETA: 56s - loss: 0.3320 - acc: 0.9012\n",
      "396/938 [===========>..................] - ETA: 56s - loss: 0.3314 - acc: 0.9014\n",
      "397/938 [===========>..................] - ETA: 56s - loss: 0.3311 - acc: 0.9014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398/938 [===========>..................] - ETA: 56s - loss: 0.3308 - acc: 0.9015\n",
      "399/938 [===========>..................] - ETA: 56s - loss: 0.3304 - acc: 0.9017\n",
      "400/938 [===========>..................] - ETA: 56s - loss: 0.3301 - acc: 0.9018\n",
      "401/938 [===========>..................] - ETA: 56s - loss: 0.3295 - acc: 0.9019\n",
      "402/938 [===========>..................] - ETA: 56s - loss: 0.3289 - acc: 0.9021\n",
      "403/938 [===========>..................] - ETA: 55s - loss: 0.3283 - acc: 0.9023\n",
      "404/938 [===========>..................] - ETA: 55s - loss: 0.3278 - acc: 0.9024\n",
      "405/938 [===========>..................] - ETA: 55s - loss: 0.3271 - acc: 0.9026\n",
      "406/938 [===========>..................] - ETA: 55s - loss: 0.3266 - acc: 0.9028\n",
      "407/938 [============>.................] - ETA: 55s - loss: 0.3259 - acc: 0.9029\n",
      "408/938 [============>.................] - ETA: 55s - loss: 0.3255 - acc: 0.9031\n",
      "409/938 [============>.................] - ETA: 55s - loss: 0.3249 - acc: 0.9033\n",
      "410/938 [============>.................] - ETA: 55s - loss: 0.3242 - acc: 0.9034\n",
      "411/938 [============>.................] - ETA: 55s - loss: 0.3241 - acc: 0.9035\n",
      "412/938 [============>.................] - ETA: 54s - loss: 0.3239 - acc: 0.9036\n",
      "413/938 [============>.................] - ETA: 54s - loss: 0.3235 - acc: 0.9037\n",
      "414/938 [============>.................] - ETA: 54s - loss: 0.3232 - acc: 0.9038\n",
      "415/938 [============>.................] - ETA: 54s - loss: 0.3232 - acc: 0.9038\n",
      "416/938 [============>.................] - ETA: 54s - loss: 0.3226 - acc: 0.9040\n",
      "417/938 [============>.................] - ETA: 54s - loss: 0.3223 - acc: 0.9040\n",
      "418/938 [============>.................] - ETA: 54s - loss: 0.3217 - acc: 0.9042\n",
      "419/938 [============>.................] - ETA: 54s - loss: 0.3211 - acc: 0.9043\n",
      "420/938 [============>.................] - ETA: 54s - loss: 0.3204 - acc: 0.9045\n",
      "421/938 [============>.................] - ETA: 53s - loss: 0.3199 - acc: 0.9047\n",
      "422/938 [============>.................] - ETA: 53s - loss: 0.3196 - acc: 0.9048\n",
      "423/938 [============>.................] - ETA: 53s - loss: 0.3190 - acc: 0.9049\n",
      "424/938 [============>.................] - ETA: 53s - loss: 0.3184 - acc: 0.9051\n",
      "425/938 [============>.................] - ETA: 53s - loss: 0.3182 - acc: 0.9052\n",
      "426/938 [============>.................] - ETA: 53s - loss: 0.3175 - acc: 0.9054\n",
      "427/938 [============>.................] - ETA: 53s - loss: 0.3173 - acc: 0.9054\n",
      "428/938 [============>.................] - ETA: 53s - loss: 0.3167 - acc: 0.9056\n",
      "429/938 [============>.................] - ETA: 53s - loss: 0.3162 - acc: 0.9057\n",
      "430/938 [============>.................] - ETA: 53s - loss: 0.3156 - acc: 0.9059\n",
      "431/938 [============>.................] - ETA: 52s - loss: 0.3152 - acc: 0.9059\n",
      "432/938 [============>.................] - ETA: 52s - loss: 0.3146 - acc: 0.9061\n",
      "433/938 [============>.................] - ETA: 52s - loss: 0.3141 - acc: 0.9062\n",
      "434/938 [============>.................] - ETA: 52s - loss: 0.3140 - acc: 0.9063\n",
      "435/938 [============>.................] - ETA: 52s - loss: 0.3135 - acc: 0.9064\n",
      "436/938 [============>.................] - ETA: 52s - loss: 0.3134 - acc: 0.9064\n",
      "437/938 [============>.................] - ETA: 52s - loss: 0.3129 - acc: 0.9065\n",
      "438/938 [=============>................] - ETA: 52s - loss: 0.3123 - acc: 0.9067\n",
      "439/938 [=============>................] - ETA: 52s - loss: 0.3118 - acc: 0.9069\n",
      "440/938 [=============>................] - ETA: 51s - loss: 0.3116 - acc: 0.9069\n",
      "441/938 [=============>................] - ETA: 51s - loss: 0.3115 - acc: 0.9070\n",
      "442/938 [=============>................] - ETA: 51s - loss: 0.3111 - acc: 0.9071\n",
      "443/938 [=============>................] - ETA: 51s - loss: 0.3113 - acc: 0.9071\n",
      "444/938 [=============>................] - ETA: 51s - loss: 0.3108 - acc: 0.9072\n",
      "445/938 [=============>................] - ETA: 51s - loss: 0.3102 - acc: 0.9074\n",
      "446/938 [=============>................] - ETA: 51s - loss: 0.3097 - acc: 0.9075\n",
      "447/938 [=============>................] - ETA: 51s - loss: 0.3093 - acc: 0.9075\n",
      "448/938 [=============>................] - ETA: 51s - loss: 0.3090 - acc: 0.9076\n",
      "449/938 [=============>................] - ETA: 50s - loss: 0.3084 - acc: 0.9078\n",
      "450/938 [=============>................] - ETA: 50s - loss: 0.3078 - acc: 0.9080\n",
      "451/938 [=============>................] - ETA: 50s - loss: 0.3073 - acc: 0.9081\n",
      "452/938 [=============>................] - ETA: 50s - loss: 0.3070 - acc: 0.9082\n",
      "453/938 [=============>................] - ETA: 50s - loss: 0.3066 - acc: 0.9083\n",
      "454/938 [=============>................] - ETA: 50s - loss: 0.3060 - acc: 0.9085\n",
      "455/938 [=============>................] - ETA: 50s - loss: 0.3057 - acc: 0.9086\n",
      "456/938 [=============>................] - ETA: 50s - loss: 0.3051 - acc: 0.9088\n",
      "457/938 [=============>................] - ETA: 50s - loss: 0.3048 - acc: 0.9089\n",
      "458/938 [=============>................] - ETA: 50s - loss: 0.3046 - acc: 0.9090\n",
      "459/938 [=============>................] - ETA: 49s - loss: 0.3041 - acc: 0.9091\n",
      "460/938 [=============>................] - ETA: 49s - loss: 0.3037 - acc: 0.9092\n",
      "461/938 [=============>................] - ETA: 49s - loss: 0.3031 - acc: 0.9093\n",
      "462/938 [=============>................] - ETA: 49s - loss: 0.3029 - acc: 0.9094\n",
      "463/938 [=============>................] - ETA: 49s - loss: 0.3024 - acc: 0.9096\n",
      "464/938 [=============>................] - ETA: 49s - loss: 0.3023 - acc: 0.9096\n",
      "465/938 [=============>................] - ETA: 49s - loss: 0.3017 - acc: 0.9098\n",
      "466/938 [=============>................] - ETA: 49s - loss: 0.3013 - acc: 0.9100\n",
      "467/938 [=============>................] - ETA: 49s - loss: 0.3015 - acc: 0.9100\n",
      "468/938 [=============>................] - ETA: 48s - loss: 0.3011 - acc: 0.9100\n",
      "469/938 [==============>...............] - ETA: 48s - loss: 0.3008 - acc: 0.9101\n",
      "470/938 [==============>...............] - ETA: 48s - loss: 0.3005 - acc: 0.9102\n",
      "471/938 [==============>...............] - ETA: 48s - loss: 0.3001 - acc: 0.9103\n",
      "472/938 [==============>...............] - ETA: 48s - loss: 0.2996 - acc: 0.9105\n",
      "473/938 [==============>...............] - ETA: 48s - loss: 0.2992 - acc: 0.9105\n",
      "474/938 [==============>...............] - ETA: 48s - loss: 0.2986 - acc: 0.9107\n",
      "475/938 [==============>...............] - ETA: 48s - loss: 0.2982 - acc: 0.9107\n",
      "476/938 [==============>...............] - ETA: 48s - loss: 0.2980 - acc: 0.9108\n",
      "477/938 [==============>...............] - ETA: 48s - loss: 0.2978 - acc: 0.9109\n",
      "478/938 [==============>...............] - ETA: 47s - loss: 0.2975 - acc: 0.9109\n",
      "479/938 [==============>...............] - ETA: 47s - loss: 0.2971 - acc: 0.9110\n",
      "480/938 [==============>...............] - ETA: 47s - loss: 0.2968 - acc: 0.9111\n",
      "481/938 [==============>...............] - ETA: 47s - loss: 0.2965 - acc: 0.9112\n",
      "482/938 [==============>...............] - ETA: 47s - loss: 0.2964 - acc: 0.9112\n",
      "483/938 [==============>...............] - ETA: 47s - loss: 0.2960 - acc: 0.9114\n",
      "484/938 [==============>...............] - ETA: 47s - loss: 0.2955 - acc: 0.9115\n",
      "485/938 [==============>...............] - ETA: 47s - loss: 0.2952 - acc: 0.9115\n",
      "486/938 [==============>...............] - ETA: 47s - loss: 0.2947 - acc: 0.9116\n",
      "487/938 [==============>...............] - ETA: 46s - loss: 0.2945 - acc: 0.9117\n",
      "488/938 [==============>...............] - ETA: 46s - loss: 0.2942 - acc: 0.9118\n",
      "489/938 [==============>...............] - ETA: 46s - loss: 0.2940 - acc: 0.9118\n",
      "490/938 [==============>...............] - ETA: 46s - loss: 0.2936 - acc: 0.9119\n",
      "491/938 [==============>...............] - ETA: 46s - loss: 0.2935 - acc: 0.9119\n",
      "492/938 [==============>...............] - ETA: 46s - loss: 0.2930 - acc: 0.9121\n",
      "493/938 [==============>...............] - ETA: 46s - loss: 0.2927 - acc: 0.9122\n",
      "494/938 [==============>...............] - ETA: 46s - loss: 0.2924 - acc: 0.9123\n",
      "495/938 [==============>...............] - ETA: 46s - loss: 0.2922 - acc: 0.9123\n",
      "496/938 [==============>...............] - ETA: 45s - loss: 0.2918 - acc: 0.9124\n",
      "497/938 [==============>...............] - ETA: 45s - loss: 0.2914 - acc: 0.9126\n",
      "498/938 [==============>...............] - ETA: 45s - loss: 0.2913 - acc: 0.9127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499/938 [==============>...............] - ETA: 45s - loss: 0.2909 - acc: 0.9128\n",
      "500/938 [==============>...............] - ETA: 45s - loss: 0.2905 - acc: 0.9129\n",
      "501/938 [===============>..............] - ETA: 45s - loss: 0.2904 - acc: 0.9129\n",
      "502/938 [===============>..............] - ETA: 45s - loss: 0.2902 - acc: 0.9130\n",
      "503/938 [===============>..............] - ETA: 45s - loss: 0.2897 - acc: 0.9131\n",
      "504/938 [===============>..............] - ETA: 45s - loss: 0.2894 - acc: 0.9132\n",
      "505/938 [===============>..............] - ETA: 45s - loss: 0.2894 - acc: 0.9131\n",
      "506/938 [===============>..............] - ETA: 44s - loss: 0.2891 - acc: 0.9132\n",
      "507/938 [===============>..............] - ETA: 44s - loss: 0.2892 - acc: 0.9132\n",
      "508/938 [===============>..............] - ETA: 44s - loss: 0.2887 - acc: 0.9134\n",
      "509/938 [===============>..............] - ETA: 44s - loss: 0.2882 - acc: 0.9136\n",
      "510/938 [===============>..............] - ETA: 44s - loss: 0.2879 - acc: 0.9136\n",
      "511/938 [===============>..............] - ETA: 44s - loss: 0.2877 - acc: 0.9136\n",
      "512/938 [===============>..............] - ETA: 44s - loss: 0.2876 - acc: 0.9137\n",
      "513/938 [===============>..............] - ETA: 44s - loss: 0.2879 - acc: 0.9136\n",
      "514/938 [===============>..............] - ETA: 44s - loss: 0.2876 - acc: 0.9137\n",
      "515/938 [===============>..............] - ETA: 44s - loss: 0.2872 - acc: 0.9137\n",
      "516/938 [===============>..............] - ETA: 43s - loss: 0.2869 - acc: 0.9139\n",
      "517/938 [===============>..............] - ETA: 43s - loss: 0.2867 - acc: 0.9139\n",
      "518/938 [===============>..............] - ETA: 43s - loss: 0.2864 - acc: 0.9139\n",
      "519/938 [===============>..............] - ETA: 43s - loss: 0.2861 - acc: 0.9140\n",
      "520/938 [===============>..............] - ETA: 43s - loss: 0.2858 - acc: 0.9141\n",
      "521/938 [===============>..............] - ETA: 43s - loss: 0.2855 - acc: 0.9142\n",
      "522/938 [===============>..............] - ETA: 43s - loss: 0.2851 - acc: 0.9143\n",
      "523/938 [===============>..............] - ETA: 43s - loss: 0.2850 - acc: 0.9144\n",
      "524/938 [===============>..............] - ETA: 43s - loss: 0.2850 - acc: 0.9144\n",
      "525/938 [===============>..............] - ETA: 42s - loss: 0.2846 - acc: 0.9145\n",
      "526/938 [===============>..............] - ETA: 42s - loss: 0.2845 - acc: 0.9146\n",
      "527/938 [===============>..............] - ETA: 42s - loss: 0.2844 - acc: 0.9147\n",
      "528/938 [===============>..............] - ETA: 42s - loss: 0.2842 - acc: 0.9147\n",
      "529/938 [===============>..............] - ETA: 42s - loss: 0.2842 - acc: 0.9147\n",
      "530/938 [===============>..............] - ETA: 42s - loss: 0.2838 - acc: 0.9148\n",
      "531/938 [===============>..............] - ETA: 42s - loss: 0.2838 - acc: 0.9148\n",
      "532/938 [================>.............] - ETA: 42s - loss: 0.2834 - acc: 0.9149\n",
      "533/938 [================>.............] - ETA: 42s - loss: 0.2830 - acc: 0.9150\n",
      "534/938 [================>.............] - ETA: 42s - loss: 0.2829 - acc: 0.9151\n",
      "535/938 [================>.............] - ETA: 41s - loss: 0.2826 - acc: 0.9151\n",
      "536/938 [================>.............] - ETA: 41s - loss: 0.2823 - acc: 0.9151\n",
      "537/938 [================>.............] - ETA: 41s - loss: 0.2821 - acc: 0.9152\n",
      "538/938 [================>.............] - ETA: 41s - loss: 0.2820 - acc: 0.9152\n",
      "539/938 [================>.............] - ETA: 41s - loss: 0.2817 - acc: 0.9153\n",
      "540/938 [================>.............] - ETA: 41s - loss: 0.2813 - acc: 0.9154\n",
      "541/938 [================>.............] - ETA: 41s - loss: 0.2814 - acc: 0.9155\n",
      "542/938 [================>.............] - ETA: 41s - loss: 0.2810 - acc: 0.9156\n",
      "543/938 [================>.............] - ETA: 41s - loss: 0.2807 - acc: 0.9157\n",
      "544/938 [================>.............] - ETA: 40s - loss: 0.2804 - acc: 0.9158\n",
      "545/938 [================>.............] - ETA: 40s - loss: 0.2800 - acc: 0.9159\n",
      "546/938 [================>.............] - ETA: 40s - loss: 0.2802 - acc: 0.9159\n",
      "547/938 [================>.............] - ETA: 40s - loss: 0.2798 - acc: 0.9160\n",
      "548/938 [================>.............] - ETA: 40s - loss: 0.2798 - acc: 0.9161\n",
      "549/938 [================>.............] - ETA: 40s - loss: 0.2797 - acc: 0.9161\n",
      "550/938 [================>.............] - ETA: 40s - loss: 0.2796 - acc: 0.9161\n",
      "551/938 [================>.............] - ETA: 40s - loss: 0.2793 - acc: 0.9161\n",
      "552/938 [================>.............] - ETA: 40s - loss: 0.2792 - acc: 0.9162\n",
      "553/938 [================>.............] - ETA: 40s - loss: 0.2790 - acc: 0.9163\n",
      "554/938 [================>.............] - ETA: 39s - loss: 0.2787 - acc: 0.9163\n",
      "555/938 [================>.............] - ETA: 39s - loss: 0.2783 - acc: 0.9165\n",
      "556/938 [================>.............] - ETA: 39s - loss: 0.2782 - acc: 0.9165\n",
      "557/938 [================>.............] - ETA: 39s - loss: 0.2779 - acc: 0.9165\n",
      "558/938 [================>.............] - ETA: 39s - loss: 0.2777 - acc: 0.9166\n",
      "559/938 [================>.............] - ETA: 39s - loss: 0.2773 - acc: 0.9167\n",
      "560/938 [================>.............] - ETA: 39s - loss: 0.2771 - acc: 0.9168\n",
      "561/938 [================>.............] - ETA: 39s - loss: 0.2768 - acc: 0.9169\n",
      "562/938 [================>.............] - ETA: 39s - loss: 0.2767 - acc: 0.9169\n",
      "563/938 [=================>............] - ETA: 38s - loss: 0.2767 - acc: 0.9169\n",
      "564/938 [=================>............] - ETA: 38s - loss: 0.2763 - acc: 0.9170\n",
      "565/938 [=================>............] - ETA: 38s - loss: 0.2760 - acc: 0.9171\n",
      "566/938 [=================>............] - ETA: 38s - loss: 0.2760 - acc: 0.9172\n",
      "567/938 [=================>............] - ETA: 38s - loss: 0.2757 - acc: 0.9172\n",
      "568/938 [=================>............] - ETA: 38s - loss: 0.2756 - acc: 0.9173\n",
      "569/938 [=================>............] - ETA: 38s - loss: 0.2754 - acc: 0.9173\n",
      "570/938 [=================>............] - ETA: 38s - loss: 0.2752 - acc: 0.9174\n",
      "571/938 [=================>............] - ETA: 38s - loss: 0.2748 - acc: 0.9175\n",
      "572/938 [=================>............] - ETA: 38s - loss: 0.2745 - acc: 0.9176\n",
      "573/938 [=================>............] - ETA: 37s - loss: 0.2742 - acc: 0.9177\n",
      "574/938 [=================>............] - ETA: 37s - loss: 0.2739 - acc: 0.9178\n",
      "575/938 [=================>............] - ETA: 37s - loss: 0.2737 - acc: 0.9178\n",
      "576/938 [=================>............] - ETA: 37s - loss: 0.2733 - acc: 0.9179\n",
      "577/938 [=================>............] - ETA: 37s - loss: 0.2731 - acc: 0.9180\n",
      "578/938 [=================>............] - ETA: 37s - loss: 0.2727 - acc: 0.9181\n",
      "579/938 [=================>............] - ETA: 37s - loss: 0.2723 - acc: 0.9182\n",
      "580/938 [=================>............] - ETA: 37s - loss: 0.2725 - acc: 0.9182\n",
      "581/938 [=================>............] - ETA: 37s - loss: 0.2722 - acc: 0.9183\n",
      "582/938 [=================>............] - ETA: 36s - loss: 0.2718 - acc: 0.9184\n",
      "583/938 [=================>............] - ETA: 36s - loss: 0.2716 - acc: 0.9185\n",
      "584/938 [=================>............] - ETA: 36s - loss: 0.2712 - acc: 0.9185\n",
      "585/938 [=================>............] - ETA: 36s - loss: 0.2711 - acc: 0.9186\n",
      "586/938 [=================>............] - ETA: 36s - loss: 0.2708 - acc: 0.9186\n",
      "587/938 [=================>............] - ETA: 36s - loss: 0.2704 - acc: 0.9188\n",
      "588/938 [=================>............] - ETA: 36s - loss: 0.2699 - acc: 0.9189\n",
      "589/938 [=================>............] - ETA: 36s - loss: 0.2696 - acc: 0.9190\n",
      "590/938 [=================>............] - ETA: 36s - loss: 0.2694 - acc: 0.9191\n",
      "591/938 [=================>............] - ETA: 36s - loss: 0.2692 - acc: 0.9191\n",
      "592/938 [=================>............] - ETA: 35s - loss: 0.2689 - acc: 0.9192\n",
      "593/938 [=================>............] - ETA: 35s - loss: 0.2685 - acc: 0.9193\n",
      "594/938 [=================>............] - ETA: 35s - loss: 0.2686 - acc: 0.9194\n",
      "595/938 [==================>...........] - ETA: 35s - loss: 0.2686 - acc: 0.9193\n",
      "596/938 [==================>...........] - ETA: 35s - loss: 0.2684 - acc: 0.9193\n",
      "597/938 [==================>...........] - ETA: 35s - loss: 0.2682 - acc: 0.9194\n",
      "598/938 [==================>...........] - ETA: 35s - loss: 0.2684 - acc: 0.9194\n",
      "599/938 [==================>...........] - ETA: 35s - loss: 0.2681 - acc: 0.9194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/938 [==================>...........] - ETA: 35s - loss: 0.2677 - acc: 0.9195\n",
      "601/938 [==================>...........] - ETA: 34s - loss: 0.2676 - acc: 0.9195\n",
      "602/938 [==================>...........] - ETA: 34s - loss: 0.2676 - acc: 0.9195\n",
      "603/938 [==================>...........] - ETA: 34s - loss: 0.2673 - acc: 0.9196\n",
      "604/938 [==================>...........] - ETA: 34s - loss: 0.2670 - acc: 0.9197\n",
      "605/938 [==================>...........] - ETA: 34s - loss: 0.2667 - acc: 0.9198\n",
      "606/938 [==================>...........] - ETA: 34s - loss: 0.2664 - acc: 0.9199\n",
      "607/938 [==================>...........] - ETA: 34s - loss: 0.2660 - acc: 0.9200\n",
      "608/938 [==================>...........] - ETA: 34s - loss: 0.2658 - acc: 0.9200\n",
      "609/938 [==================>...........] - ETA: 34s - loss: 0.2660 - acc: 0.9201\n",
      "610/938 [==================>...........] - ETA: 34s - loss: 0.2659 - acc: 0.9201\n",
      "611/938 [==================>...........] - ETA: 33s - loss: 0.2655 - acc: 0.9202\n",
      "612/938 [==================>...........] - ETA: 33s - loss: 0.2652 - acc: 0.9203\n",
      "613/938 [==================>...........] - ETA: 33s - loss: 0.2650 - acc: 0.9203\n",
      "614/938 [==================>...........] - ETA: 33s - loss: 0.2647 - acc: 0.9204\n",
      "615/938 [==================>...........] - ETA: 33s - loss: 0.2645 - acc: 0.9204\n",
      "616/938 [==================>...........] - ETA: 33s - loss: 0.2642 - acc: 0.9204\n",
      "617/938 [==================>...........] - ETA: 33s - loss: 0.2642 - acc: 0.9204\n",
      "618/938 [==================>...........] - ETA: 33s - loss: 0.2641 - acc: 0.9204\n",
      "619/938 [==================>...........] - ETA: 33s - loss: 0.2638 - acc: 0.9205\n",
      "620/938 [==================>...........] - ETA: 32s - loss: 0.2636 - acc: 0.9206\n",
      "621/938 [==================>...........] - ETA: 32s - loss: 0.2636 - acc: 0.9206\n",
      "622/938 [==================>...........] - ETA: 32s - loss: 0.2634 - acc: 0.9206\n",
      "623/938 [==================>...........] - ETA: 32s - loss: 0.2632 - acc: 0.9207\n",
      "624/938 [==================>...........] - ETA: 32s - loss: 0.2629 - acc: 0.9208\n",
      "625/938 [==================>...........] - ETA: 32s - loss: 0.2627 - acc: 0.9208\n",
      "626/938 [===================>..........] - ETA: 32s - loss: 0.2623 - acc: 0.9209\n",
      "627/938 [===================>..........] - ETA: 32s - loss: 0.2620 - acc: 0.9210\n",
      "628/938 [===================>..........] - ETA: 32s - loss: 0.2620 - acc: 0.9210\n",
      "629/938 [===================>..........] - ETA: 32s - loss: 0.2619 - acc: 0.9211\n",
      "630/938 [===================>..........] - ETA: 31s - loss: 0.2621 - acc: 0.9211\n",
      "631/938 [===================>..........] - ETA: 31s - loss: 0.2618 - acc: 0.9212\n",
      "632/938 [===================>..........] - ETA: 31s - loss: 0.2617 - acc: 0.9212\n",
      "633/938 [===================>..........] - ETA: 31s - loss: 0.2615 - acc: 0.9212\n",
      "634/938 [===================>..........] - ETA: 31s - loss: 0.2616 - acc: 0.9212\n",
      "635/938 [===================>..........] - ETA: 31s - loss: 0.2616 - acc: 0.9212\n",
      "636/938 [===================>..........] - ETA: 31s - loss: 0.2614 - acc: 0.9213\n",
      "637/938 [===================>..........] - ETA: 31s - loss: 0.2613 - acc: 0.9213\n",
      "638/938 [===================>..........] - ETA: 31s - loss: 0.2612 - acc: 0.9213\n",
      "639/938 [===================>..........] - ETA: 30s - loss: 0.2608 - acc: 0.9214\n",
      "640/938 [===================>..........] - ETA: 30s - loss: 0.2606 - acc: 0.9215\n",
      "641/938 [===================>..........] - ETA: 30s - loss: 0.2603 - acc: 0.9215\n",
      "642/938 [===================>..........] - ETA: 30s - loss: 0.2600 - acc: 0.9216\n",
      "643/938 [===================>..........] - ETA: 30s - loss: 0.2598 - acc: 0.9217\n",
      "644/938 [===================>..........] - ETA: 30s - loss: 0.2597 - acc: 0.9217\n",
      "645/938 [===================>..........] - ETA: 30s - loss: 0.2595 - acc: 0.9217\n",
      "646/938 [===================>..........] - ETA: 30s - loss: 0.2593 - acc: 0.9217\n",
      "647/938 [===================>..........] - ETA: 30s - loss: 0.2591 - acc: 0.9218\n",
      "648/938 [===================>..........] - ETA: 30s - loss: 0.2588 - acc: 0.9218\n",
      "649/938 [===================>..........] - ETA: 29s - loss: 0.2585 - acc: 0.9219\n",
      "650/938 [===================>..........] - ETA: 29s - loss: 0.2582 - acc: 0.9220\n",
      "651/938 [===================>..........] - ETA: 29s - loss: 0.2579 - acc: 0.9221\n",
      "652/938 [===================>..........] - ETA: 29s - loss: 0.2577 - acc: 0.9222\n",
      "653/938 [===================>..........] - ETA: 29s - loss: 0.2575 - acc: 0.9222\n",
      "654/938 [===================>..........] - ETA: 29s - loss: 0.2572 - acc: 0.9223\n",
      "655/938 [===================>..........] - ETA: 29s - loss: 0.2569 - acc: 0.9224\n",
      "656/938 [===================>..........] - ETA: 29s - loss: 0.2569 - acc: 0.9224\n",
      "657/938 [====================>.........] - ETA: 29s - loss: 0.2567 - acc: 0.9224\n",
      "658/938 [====================>.........] - ETA: 29s - loss: 0.2564 - acc: 0.9225\n",
      "659/938 [====================>.........] - ETA: 28s - loss: 0.2561 - acc: 0.9226\n",
      "660/938 [====================>.........] - ETA: 28s - loss: 0.2558 - acc: 0.9227\n",
      "661/938 [====================>.........] - ETA: 28s - loss: 0.2556 - acc: 0.9228\n",
      "662/938 [====================>.........] - ETA: 28s - loss: 0.2553 - acc: 0.9228\n",
      "663/938 [====================>.........] - ETA: 28s - loss: 0.2551 - acc: 0.9229\n",
      "664/938 [====================>.........] - ETA: 28s - loss: 0.2549 - acc: 0.9230\n",
      "665/938 [====================>.........] - ETA: 28s - loss: 0.2547 - acc: 0.9230\n",
      "666/938 [====================>.........] - ETA: 28s - loss: 0.2544 - acc: 0.9231\n",
      "667/938 [====================>.........] - ETA: 28s - loss: 0.2541 - acc: 0.9232\n",
      "668/938 [====================>.........] - ETA: 27s - loss: 0.2538 - acc: 0.9233\n",
      "669/938 [====================>.........] - ETA: 27s - loss: 0.2535 - acc: 0.9233\n",
      "670/938 [====================>.........] - ETA: 27s - loss: 0.2534 - acc: 0.9234\n",
      "671/938 [====================>.........] - ETA: 27s - loss: 0.2532 - acc: 0.9235\n",
      "672/938 [====================>.........] - ETA: 27s - loss: 0.2530 - acc: 0.9235\n",
      "673/938 [====================>.........] - ETA: 27s - loss: 0.2529 - acc: 0.9236\n",
      "674/938 [====================>.........] - ETA: 27s - loss: 0.2528 - acc: 0.9236\n",
      "675/938 [====================>.........] - ETA: 27s - loss: 0.2527 - acc: 0.9237\n",
      "676/938 [====================>.........] - ETA: 27s - loss: 0.2526 - acc: 0.9237\n",
      "677/938 [====================>.........] - ETA: 27s - loss: 0.2523 - acc: 0.9238\n",
      "678/938 [====================>.........] - ETA: 26s - loss: 0.2521 - acc: 0.9239\n",
      "679/938 [====================>.........] - ETA: 26s - loss: 0.2518 - acc: 0.9240\n",
      "680/938 [====================>.........] - ETA: 26s - loss: 0.2517 - acc: 0.9241\n",
      "681/938 [====================>.........] - ETA: 26s - loss: 0.2513 - acc: 0.9241\n",
      "682/938 [====================>.........] - ETA: 26s - loss: 0.2511 - acc: 0.9242\n",
      "683/938 [====================>.........] - ETA: 26s - loss: 0.2508 - acc: 0.9243\n",
      "684/938 [====================>.........] - ETA: 26s - loss: 0.2506 - acc: 0.9244\n",
      "685/938 [====================>.........] - ETA: 26s - loss: 0.2504 - acc: 0.9245\n",
      "686/938 [====================>.........] - ETA: 26s - loss: 0.2503 - acc: 0.9244\n",
      "687/938 [====================>.........] - ETA: 25s - loss: 0.2500 - acc: 0.9245\n",
      "688/938 [=====================>........] - ETA: 25s - loss: 0.2498 - acc: 0.9246\n",
      "689/938 [=====================>........] - ETA: 25s - loss: 0.2495 - acc: 0.9247\n",
      "690/938 [=====================>........] - ETA: 25s - loss: 0.2493 - acc: 0.9248\n",
      "691/938 [=====================>........] - ETA: 25s - loss: 0.2492 - acc: 0.9248\n",
      "692/938 [=====================>........] - ETA: 25s - loss: 0.2491 - acc: 0.9248\n",
      "693/938 [=====================>........] - ETA: 25s - loss: 0.2493 - acc: 0.9248\n",
      "694/938 [=====================>........] - ETA: 25s - loss: 0.2491 - acc: 0.9248\n",
      "695/938 [=====================>........] - ETA: 25s - loss: 0.2488 - acc: 0.9249\n",
      "696/938 [=====================>........] - ETA: 25s - loss: 0.2487 - acc: 0.9249\n",
      "697/938 [=====================>........] - ETA: 24s - loss: 0.2485 - acc: 0.9249\n",
      "698/938 [=====================>........] - ETA: 24s - loss: 0.2483 - acc: 0.9250\n",
      "699/938 [=====================>........] - ETA: 24s - loss: 0.2480 - acc: 0.9250\n",
      "700/938 [=====================>........] - ETA: 24s - loss: 0.2481 - acc: 0.9250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "701/938 [=====================>........] - ETA: 24s - loss: 0.2478 - acc: 0.9250\n",
      "702/938 [=====================>........] - ETA: 24s - loss: 0.2477 - acc: 0.9251\n",
      "703/938 [=====================>........] - ETA: 24s - loss: 0.2476 - acc: 0.9251\n",
      "704/938 [=====================>........] - ETA: 24s - loss: 0.2473 - acc: 0.9252\n",
      "705/938 [=====================>........] - ETA: 24s - loss: 0.2471 - acc: 0.9252\n",
      "706/938 [=====================>........] - ETA: 24s - loss: 0.2468 - acc: 0.9253\n",
      "707/938 [=====================>........] - ETA: 23s - loss: 0.2466 - acc: 0.9253\n",
      "708/938 [=====================>........] - ETA: 23s - loss: 0.2466 - acc: 0.9254\n",
      "709/938 [=====================>........] - ETA: 23s - loss: 0.2463 - acc: 0.9254\n",
      "710/938 [=====================>........] - ETA: 23s - loss: 0.2462 - acc: 0.9254\n",
      "711/938 [=====================>........] - ETA: 23s - loss: 0.2460 - acc: 0.9255\n",
      "712/938 [=====================>........] - ETA: 23s - loss: 0.2458 - acc: 0.9256\n",
      "713/938 [=====================>........] - ETA: 23s - loss: 0.2456 - acc: 0.9256\n",
      "714/938 [=====================>........] - ETA: 23s - loss: 0.2456 - acc: 0.9256\n",
      "715/938 [=====================>........] - ETA: 23s - loss: 0.2457 - acc: 0.9256\n",
      "716/938 [=====================>........] - ETA: 22s - loss: 0.2455 - acc: 0.9256\n",
      "717/938 [=====================>........] - ETA: 22s - loss: 0.2453 - acc: 0.9257\n",
      "718/938 [=====================>........] - ETA: 22s - loss: 0.2450 - acc: 0.9258\n",
      "719/938 [=====================>........] - ETA: 22s - loss: 0.2449 - acc: 0.9258\n",
      "720/938 [======================>.......] - ETA: 22s - loss: 0.2447 - acc: 0.9259\n",
      "721/938 [======================>.......] - ETA: 22s - loss: 0.2444 - acc: 0.9260\n",
      "722/938 [======================>.......] - ETA: 22s - loss: 0.2446 - acc: 0.9260\n",
      "723/938 [======================>.......] - ETA: 22s - loss: 0.2443 - acc: 0.9261\n",
      "724/938 [======================>.......] - ETA: 22s - loss: 0.2440 - acc: 0.9261\n",
      "725/938 [======================>.......] - ETA: 22s - loss: 0.2440 - acc: 0.9261\n",
      "726/938 [======================>.......] - ETA: 21s - loss: 0.2440 - acc: 0.9261\n",
      "727/938 [======================>.......] - ETA: 21s - loss: 0.2438 - acc: 0.9261\n",
      "728/938 [======================>.......] - ETA: 21s - loss: 0.2438 - acc: 0.9261\n",
      "729/938 [======================>.......] - ETA: 21s - loss: 0.2437 - acc: 0.9262\n",
      "730/938 [======================>.......] - ETA: 21s - loss: 0.2434 - acc: 0.9262\n",
      "731/938 [======================>.......] - ETA: 21s - loss: 0.2431 - acc: 0.9263\n",
      "732/938 [======================>.......] - ETA: 21s - loss: 0.2429 - acc: 0.9264\n",
      "733/938 [======================>.......] - ETA: 21s - loss: 0.2426 - acc: 0.9265\n",
      "734/938 [======================>.......] - ETA: 21s - loss: 0.2424 - acc: 0.9265\n",
      "735/938 [======================>.......] - ETA: 20s - loss: 0.2422 - acc: 0.9266\n",
      "736/938 [======================>.......] - ETA: 20s - loss: 0.2420 - acc: 0.9266\n",
      "737/938 [======================>.......] - ETA: 20s - loss: 0.2419 - acc: 0.9266\n",
      "738/938 [======================>.......] - ETA: 20s - loss: 0.2418 - acc: 0.9266\n",
      "739/938 [======================>.......] - ETA: 20s - loss: 0.2416 - acc: 0.9267\n",
      "740/938 [======================>.......] - ETA: 20s - loss: 0.2413 - acc: 0.9268\n",
      "741/938 [======================>.......] - ETA: 20s - loss: 0.2411 - acc: 0.9269\n",
      "742/938 [======================>.......] - ETA: 20s - loss: 0.2409 - acc: 0.9269\n",
      "743/938 [======================>.......] - ETA: 20s - loss: 0.2407 - acc: 0.9270\n",
      "744/938 [======================>.......] - ETA: 20s - loss: 0.2405 - acc: 0.9271\n",
      "745/938 [======================>.......] - ETA: 19s - loss: 0.2402 - acc: 0.9272\n",
      "746/938 [======================>.......] - ETA: 19s - loss: 0.2400 - acc: 0.9272\n",
      "747/938 [======================>.......] - ETA: 19s - loss: 0.2397 - acc: 0.9273\n",
      "748/938 [======================>.......] - ETA: 19s - loss: 0.2395 - acc: 0.9274\n",
      "749/938 [======================>.......] - ETA: 19s - loss: 0.2396 - acc: 0.9274\n",
      "750/938 [======================>.......] - ETA: 19s - loss: 0.2394 - acc: 0.9274\n",
      "751/938 [=======================>......] - ETA: 19s - loss: 0.2392 - acc: 0.9275\n",
      "752/938 [=======================>......] - ETA: 19s - loss: 0.2389 - acc: 0.9275\n",
      "753/938 [=======================>......] - ETA: 19s - loss: 0.2387 - acc: 0.9276\n",
      "754/938 [=======================>......] - ETA: 19s - loss: 0.2386 - acc: 0.9277\n",
      "755/938 [=======================>......] - ETA: 18s - loss: 0.2385 - acc: 0.9277\n",
      "756/938 [=======================>......] - ETA: 18s - loss: 0.2382 - acc: 0.9278\n",
      "757/938 [=======================>......] - ETA: 18s - loss: 0.2379 - acc: 0.9279\n",
      "758/938 [=======================>......] - ETA: 18s - loss: 0.2378 - acc: 0.9279\n",
      "759/938 [=======================>......] - ETA: 18s - loss: 0.2376 - acc: 0.9280\n",
      "760/938 [=======================>......] - ETA: 18s - loss: 0.2373 - acc: 0.9280\n",
      "761/938 [=======================>......] - ETA: 18s - loss: 0.2372 - acc: 0.9281\n",
      "762/938 [=======================>......] - ETA: 18s - loss: 0.2369 - acc: 0.9281\n",
      "763/938 [=======================>......] - ETA: 18s - loss: 0.2366 - acc: 0.9282\n",
      "764/938 [=======================>......] - ETA: 17s - loss: 0.2365 - acc: 0.9283\n",
      "765/938 [=======================>......] - ETA: 17s - loss: 0.2364 - acc: 0.9283\n",
      "766/938 [=======================>......] - ETA: 17s - loss: 0.2361 - acc: 0.9284\n",
      "767/938 [=======================>......] - ETA: 17s - loss: 0.2361 - acc: 0.9284\n",
      "768/938 [=======================>......] - ETA: 17s - loss: 0.2359 - acc: 0.9285\n",
      "769/938 [=======================>......] - ETA: 17s - loss: 0.2359 - acc: 0.9285\n",
      "770/938 [=======================>......] - ETA: 17s - loss: 0.2357 - acc: 0.9285\n",
      "771/938 [=======================>......] - ETA: 17s - loss: 0.2356 - acc: 0.9285\n",
      "772/938 [=======================>......] - ETA: 17s - loss: 0.2354 - acc: 0.9286\n",
      "773/938 [=======================>......] - ETA: 17s - loss: 0.2352 - acc: 0.9286\n",
      "774/938 [=======================>......] - ETA: 16s - loss: 0.2350 - acc: 0.9287\n",
      "775/938 [=======================>......] - ETA: 16s - loss: 0.2348 - acc: 0.9287\n",
      "776/938 [=======================>......] - ETA: 16s - loss: 0.2346 - acc: 0.9288\n",
      "777/938 [=======================>......] - ETA: 16s - loss: 0.2345 - acc: 0.9288\n",
      "778/938 [=======================>......] - ETA: 16s - loss: 0.2343 - acc: 0.9288\n",
      "779/938 [=======================>......] - ETA: 16s - loss: 0.2341 - acc: 0.9289\n",
      "780/938 [=======================>......] - ETA: 16s - loss: 0.2340 - acc: 0.9289\n",
      "781/938 [=======================>......] - ETA: 16s - loss: 0.2338 - acc: 0.9290\n",
      "782/938 [========================>.....] - ETA: 16s - loss: 0.2336 - acc: 0.9290\n",
      "783/938 [========================>.....] - ETA: 16s - loss: 0.2335 - acc: 0.9290\n",
      "784/938 [========================>.....] - ETA: 15s - loss: 0.2335 - acc: 0.9290\n",
      "785/938 [========================>.....] - ETA: 15s - loss: 0.2334 - acc: 0.9290\n",
      "786/938 [========================>.....] - ETA: 15s - loss: 0.2333 - acc: 0.9291\n",
      "787/938 [========================>.....] - ETA: 15s - loss: 0.2330 - acc: 0.9291\n",
      "788/938 [========================>.....] - ETA: 15s - loss: 0.2329 - acc: 0.9292\n",
      "789/938 [========================>.....] - ETA: 15s - loss: 0.2327 - acc: 0.9292\n",
      "790/938 [========================>.....] - ETA: 15s - loss: 0.2325 - acc: 0.9293\n",
      "791/938 [========================>.....] - ETA: 15s - loss: 0.2324 - acc: 0.9293\n",
      "792/938 [========================>.....] - ETA: 15s - loss: 0.2323 - acc: 0.9293\n",
      "793/938 [========================>.....] - ETA: 14s - loss: 0.2321 - acc: 0.9294\n",
      "794/938 [========================>.....] - ETA: 14s - loss: 0.2319 - acc: 0.9295\n",
      "795/938 [========================>.....] - ETA: 14s - loss: 0.2317 - acc: 0.9295\n",
      "796/938 [========================>.....] - ETA: 14s - loss: 0.2315 - acc: 0.9296\n",
      "797/938 [========================>.....] - ETA: 14s - loss: 0.2314 - acc: 0.9296\n",
      "798/938 [========================>.....] - ETA: 14s - loss: 0.2313 - acc: 0.9296\n",
      "799/938 [========================>.....] - ETA: 14s - loss: 0.2311 - acc: 0.9297\n",
      "800/938 [========================>.....] - ETA: 14s - loss: 0.2311 - acc: 0.9297\n",
      "801/938 [========================>.....] - ETA: 14s - loss: 0.2312 - acc: 0.9297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "802/938 [========================>.....] - ETA: 14s - loss: 0.2309 - acc: 0.9298\n",
      "803/938 [========================>.....] - ETA: 13s - loss: 0.2309 - acc: 0.9298\n",
      "804/938 [========================>.....] - ETA: 13s - loss: 0.2307 - acc: 0.9299\n",
      "805/938 [========================>.....] - ETA: 13s - loss: 0.2305 - acc: 0.9299\n",
      "806/938 [========================>.....] - ETA: 13s - loss: 0.2306 - acc: 0.9299\n",
      "807/938 [========================>.....] - ETA: 13s - loss: 0.2303 - acc: 0.9300\n",
      "808/938 [========================>.....] - ETA: 13s - loss: 0.2301 - acc: 0.9301\n",
      "809/938 [========================>.....] - ETA: 13s - loss: 0.2301 - acc: 0.9301\n",
      "810/938 [========================>.....] - ETA: 13s - loss: 0.2299 - acc: 0.9302\n",
      "811/938 [========================>.....] - ETA: 13s - loss: 0.2297 - acc: 0.9302\n",
      "812/938 [========================>.....] - ETA: 13s - loss: 0.2296 - acc: 0.9303\n",
      "813/938 [=========================>....] - ETA: 12s - loss: 0.2296 - acc: 0.9303\n",
      "814/938 [=========================>....] - ETA: 12s - loss: 0.2295 - acc: 0.9303\n",
      "815/938 [=========================>....] - ETA: 12s - loss: 0.2292 - acc: 0.9303\n",
      "816/938 [=========================>....] - ETA: 12s - loss: 0.2290 - acc: 0.9304\n",
      "817/938 [=========================>....] - ETA: 12s - loss: 0.2287 - acc: 0.9305\n",
      "818/938 [=========================>....] - ETA: 12s - loss: 0.2286 - acc: 0.9305\n",
      "819/938 [=========================>....] - ETA: 12s - loss: 0.2285 - acc: 0.9306\n",
      "820/938 [=========================>....] - ETA: 12s - loss: 0.2284 - acc: 0.9306\n",
      "821/938 [=========================>....] - ETA: 12s - loss: 0.2284 - acc: 0.9306\n",
      "822/938 [=========================>....] - ETA: 11s - loss: 0.2281 - acc: 0.9306\n",
      "823/938 [=========================>....] - ETA: 11s - loss: 0.2279 - acc: 0.9307\n",
      "824/938 [=========================>....] - ETA: 11s - loss: 0.2277 - acc: 0.9307\n",
      "825/938 [=========================>....] - ETA: 11s - loss: 0.2277 - acc: 0.9308\n",
      "826/938 [=========================>....] - ETA: 11s - loss: 0.2275 - acc: 0.9308\n",
      "827/938 [=========================>....] - ETA: 11s - loss: 0.2273 - acc: 0.9308\n",
      "828/938 [=========================>....] - ETA: 11s - loss: 0.2271 - acc: 0.9309\n",
      "829/938 [=========================>....] - ETA: 11s - loss: 0.2268 - acc: 0.9310\n",
      "830/938 [=========================>....] - ETA: 11s - loss: 0.2266 - acc: 0.9311\n",
      "831/938 [=========================>....] - ETA: 11s - loss: 0.2267 - acc: 0.9311\n",
      "832/938 [=========================>....] - ETA: 10s - loss: 0.2266 - acc: 0.9312\n",
      "833/938 [=========================>....] - ETA: 10s - loss: 0.2267 - acc: 0.9311\n",
      "834/938 [=========================>....] - ETA: 10s - loss: 0.2266 - acc: 0.9312\n",
      "835/938 [=========================>....] - ETA: 10s - loss: 0.2265 - acc: 0.9312\n",
      "836/938 [=========================>....] - ETA: 10s - loss: 0.2264 - acc: 0.9312\n",
      "837/938 [=========================>....] - ETA: 10s - loss: 0.2262 - acc: 0.9312\n",
      "838/938 [=========================>....] - ETA: 10s - loss: 0.2260 - acc: 0.9313\n",
      "839/938 [=========================>....] - ETA: 10s - loss: 0.2259 - acc: 0.9313\n",
      "840/938 [=========================>....] - ETA: 10s - loss: 0.2261 - acc: 0.9313\n",
      "841/938 [=========================>....] - ETA: 10s - loss: 0.2259 - acc: 0.9314\n",
      "842/938 [=========================>....] - ETA: 9s - loss: 0.2257 - acc: 0.9315 \n",
      "843/938 [=========================>....] - ETA: 9s - loss: 0.2256 - acc: 0.9315\n",
      "844/938 [=========================>....] - ETA: 9s - loss: 0.2255 - acc: 0.9315\n",
      "845/938 [==========================>...] - ETA: 9s - loss: 0.2254 - acc: 0.9315\n",
      "846/938 [==========================>...] - ETA: 9s - loss: 0.2253 - acc: 0.9316\n",
      "847/938 [==========================>...] - ETA: 9s - loss: 0.2253 - acc: 0.9316\n",
      "848/938 [==========================>...] - ETA: 9s - loss: 0.2252 - acc: 0.9316\n",
      "849/938 [==========================>...] - ETA: 9s - loss: 0.2251 - acc: 0.9316\n",
      "850/938 [==========================>...] - ETA: 9s - loss: 0.2250 - acc: 0.9316\n",
      "851/938 [==========================>...] - ETA: 8s - loss: 0.2249 - acc: 0.9317\n",
      "852/938 [==========================>...] - ETA: 8s - loss: 0.2248 - acc: 0.9317\n",
      "853/938 [==========================>...] - ETA: 8s - loss: 0.2249 - acc: 0.9317\n",
      "854/938 [==========================>...] - ETA: 8s - loss: 0.2246 - acc: 0.9318\n",
      "855/938 [==========================>...] - ETA: 8s - loss: 0.2245 - acc: 0.9318\n",
      "856/938 [==========================>...] - ETA: 8s - loss: 0.2244 - acc: 0.9318\n",
      "857/938 [==========================>...] - ETA: 8s - loss: 0.2242 - acc: 0.9319\n",
      "858/938 [==========================>...] - ETA: 8s - loss: 0.2240 - acc: 0.9320\n",
      "859/938 [==========================>...] - ETA: 8s - loss: 0.2239 - acc: 0.9320\n",
      "860/938 [==========================>...] - ETA: 8s - loss: 0.2238 - acc: 0.9321\n",
      "861/938 [==========================>...] - ETA: 7s - loss: 0.2237 - acc: 0.9321\n",
      "862/938 [==========================>...] - ETA: 7s - loss: 0.2236 - acc: 0.9321\n",
      "863/938 [==========================>...] - ETA: 7s - loss: 0.2235 - acc: 0.9322\n",
      "864/938 [==========================>...] - ETA: 7s - loss: 0.2234 - acc: 0.9322\n",
      "865/938 [==========================>...] - ETA: 7s - loss: 0.2236 - acc: 0.9322\n",
      "866/938 [==========================>...] - ETA: 7s - loss: 0.2235 - acc: 0.9323\n",
      "867/938 [==========================>...] - ETA: 7s - loss: 0.2233 - acc: 0.9323\n",
      "868/938 [==========================>...] - ETA: 7s - loss: 0.2232 - acc: 0.9324\n",
      "869/938 [==========================>...] - ETA: 7s - loss: 0.2231 - acc: 0.9324\n",
      "870/938 [==========================>...] - ETA: 7s - loss: 0.2230 - acc: 0.9324\n",
      "871/938 [==========================>...] - ETA: 6s - loss: 0.2229 - acc: 0.9324\n",
      "872/938 [==========================>...] - ETA: 6s - loss: 0.2227 - acc: 0.9325\n",
      "873/938 [==========================>...] - ETA: 6s - loss: 0.2225 - acc: 0.9325\n",
      "874/938 [==========================>...] - ETA: 6s - loss: 0.2225 - acc: 0.9325\n",
      "875/938 [==========================>...] - ETA: 6s - loss: 0.2225 - acc: 0.9326\n",
      "876/938 [===========================>..] - ETA: 6s - loss: 0.2224 - acc: 0.9326\n",
      "877/938 [===========================>..] - ETA: 6s - loss: 0.2222 - acc: 0.9327\n",
      "878/938 [===========================>..] - ETA: 6s - loss: 0.2220 - acc: 0.9327\n",
      "879/938 [===========================>..] - ETA: 6s - loss: 0.2219 - acc: 0.9328\n",
      "880/938 [===========================>..] - ETA: 5s - loss: 0.2218 - acc: 0.9328\n",
      "881/938 [===========================>..] - ETA: 5s - loss: 0.2217 - acc: 0.9328\n",
      "882/938 [===========================>..] - ETA: 5s - loss: 0.2216 - acc: 0.9329\n",
      "883/938 [===========================>..] - ETA: 5s - loss: 0.2213 - acc: 0.9330\n",
      "884/938 [===========================>..] - ETA: 5s - loss: 0.2211 - acc: 0.9330\n",
      "885/938 [===========================>..] - ETA: 5s - loss: 0.2210 - acc: 0.9330\n",
      "886/938 [===========================>..] - ETA: 5s - loss: 0.2208 - acc: 0.9331\n",
      "887/938 [===========================>..] - ETA: 5s - loss: 0.2207 - acc: 0.9331\n",
      "888/938 [===========================>..] - ETA: 5s - loss: 0.2207 - acc: 0.9331\n",
      "889/938 [===========================>..] - ETA: 5s - loss: 0.2207 - acc: 0.9332\n",
      "890/938 [===========================>..] - ETA: 4s - loss: 0.2206 - acc: 0.9332\n",
      "891/938 [===========================>..] - ETA: 4s - loss: 0.2205 - acc: 0.9332\n",
      "892/938 [===========================>..] - ETA: 4s - loss: 0.2204 - acc: 0.9332\n",
      "893/938 [===========================>..] - ETA: 4s - loss: 0.2202 - acc: 0.9333\n",
      "894/938 [===========================>..] - ETA: 4s - loss: 0.2200 - acc: 0.9334\n",
      "895/938 [===========================>..] - ETA: 4s - loss: 0.2200 - acc: 0.9334\n",
      "896/938 [===========================>..] - ETA: 4s - loss: 0.2199 - acc: 0.9334\n",
      "897/938 [===========================>..] - ETA: 4s - loss: 0.2198 - acc: 0.9335\n",
      "898/938 [===========================>..] - ETA: 4s - loss: 0.2197 - acc: 0.9335\n",
      "899/938 [===========================>..] - ETA: 4s - loss: 0.2196 - acc: 0.9335\n",
      "900/938 [===========================>..] - ETA: 3s - loss: 0.2194 - acc: 0.9336\n",
      "901/938 [===========================>..] - ETA: 3s - loss: 0.2194 - acc: 0.9336\n",
      "902/938 [===========================>..] - ETA: 3s - loss: 0.2192 - acc: 0.9337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "903/938 [===========================>..] - ETA: 3s - loss: 0.2190 - acc: 0.9337\n",
      "904/938 [===========================>..] - ETA: 3s - loss: 0.2188 - acc: 0.9338\n",
      "905/938 [===========================>..] - ETA: 3s - loss: 0.2186 - acc: 0.9338\n",
      "906/938 [===========================>..] - ETA: 3s - loss: 0.2184 - acc: 0.9339\n",
      "907/938 [============================>.] - ETA: 3s - loss: 0.2182 - acc: 0.9340\n",
      "908/938 [============================>.] - ETA: 3s - loss: 0.2181 - acc: 0.9340\n",
      "909/938 [============================>.] - ETA: 2s - loss: 0.2179 - acc: 0.9341\n",
      "910/938 [============================>.] - ETA: 2s - loss: 0.2177 - acc: 0.9341\n",
      "911/938 [============================>.] - ETA: 2s - loss: 0.2177 - acc: 0.9341\n",
      "912/938 [============================>.] - ETA: 2s - loss: 0.2177 - acc: 0.9341\n",
      "913/938 [============================>.] - ETA: 2s - loss: 0.2176 - acc: 0.9341\n",
      "914/938 [============================>.] - ETA: 2s - loss: 0.2174 - acc: 0.9342\n",
      "915/938 [============================>.] - ETA: 2s - loss: 0.2172 - acc: 0.9342\n",
      "916/938 [============================>.] - ETA: 2s - loss: 0.2171 - acc: 0.9342\n",
      "917/938 [============================>.] - ETA: 2s - loss: 0.2169 - acc: 0.9343\n",
      "918/938 [============================>.] - ETA: 2s - loss: 0.2168 - acc: 0.9344\n",
      "919/938 [============================>.] - ETA: 1s - loss: 0.2166 - acc: 0.9344\n",
      "920/938 [============================>.] - ETA: 1s - loss: 0.2165 - acc: 0.9344\n",
      "921/938 [============================>.] - ETA: 1s - loss: 0.2164 - acc: 0.9345\n",
      "922/938 [============================>.] - ETA: 1s - loss: 0.2162 - acc: 0.9345\n",
      "923/938 [============================>.] - ETA: 1s - loss: 0.2164 - acc: 0.9345\n",
      "924/938 [============================>.] - ETA: 1s - loss: 0.2163 - acc: 0.9345\n",
      "925/938 [============================>.] - ETA: 1s - loss: 0.2163 - acc: 0.9345\n",
      "926/938 [============================>.] - ETA: 1s - loss: 0.2163 - acc: 0.9345\n",
      "927/938 [============================>.] - ETA: 1s - loss: 0.2161 - acc: 0.9346\n",
      "928/938 [============================>.] - ETA: 1s - loss: 0.2160 - acc: 0.9346\n",
      "929/938 [============================>.] - ETA: 0s - loss: 0.2158 - acc: 0.9347\n",
      "930/938 [============================>.] - ETA: 0s - loss: 0.2157 - acc: 0.9347\n",
      "931/938 [============================>.] - ETA: 0s - loss: 0.2156 - acc: 0.9347\n",
      "932/938 [============================>.] - ETA: 0s - loss: 0.2156 - acc: 0.9347\n",
      "933/938 [============================>.] - ETA: 0s - loss: 0.2154 - acc: 0.9348\n",
      "934/938 [============================>.] - ETA: 0s - loss: 0.2152 - acc: 0.9349\n",
      "935/938 [============================>.] - ETA: 0s - loss: 0.2150 - acc: 0.9349\n",
      "936/938 [============================>.] - ETA: 0s - loss: 0.2149 - acc: 0.9349\n",
      "937/938 [============================>.] - ETA: 0s - loss: 0.2148 - acc: 0.9350\n",
      "938/938 [==============================] - 105s 112ms/step - loss: 0.2147 - acc: 0.9350 - val_loss: 0.0262 - val_acc: 0.9918\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb91275aac8>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(batches, steps_per_epoch=int(math.ceil(batches.n / batches.batch_size)), epochs=1, verbose=2,\n",
    "                 validation_data=test_batches, validation_steps=int(math.ceil(test_batches.n/test_batches.batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      " - 102s - loss: 0.0911 - acc: 0.9714 - val_loss: 0.0244 - val_acc: 0.9919\n",
      "Epoch 2/4\n",
      " - 102s - loss: 0.0766 - acc: 0.9767 - val_loss: 0.0208 - val_acc: 0.9932\n",
      "Epoch 3/4\n",
      " - 102s - loss: 0.0660 - acc: 0.9799 - val_loss: 0.0225 - val_acc: 0.9931\n",
      "Epoch 4/4\n",
      " - 102s - loss: 0.0597 - acc: 0.9811 - val_loss: 0.0310 - val_acc: 0.9906\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb91275aa90>"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(batches, steps_per_epoch=int(math.ceil(batches.n / batches.batch_size)), epochs=4, verbose=2,\n",
    "                 validation_data=test_batches, validation_steps=int(math.ceil(test_batches.n/test_batches.batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-151-cb6b2ee8c8e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model.fit_generator(batches, steps_per_epoch=int(math.ceil(batches.n / batches.batch_size)), epochs=12, verbose=2,\n\u001b[0;32m----> 2\u001b[0;31m                  validation_data=test_batches, validation_steps=int(math.ceil(test_batches.n/test_batches.batch_size)))\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1154\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m                                         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1156\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2075\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2076\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2077\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2078\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2079\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1795\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1796\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1797\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1798\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1799\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2330\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2331\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2332\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2333\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit_generator(batches, steps_per_epoch=int(math.ceil(batches.n / batches.batch_size)), epochs=12, verbose=2,\n",
    "                 validation_data=test_batches, validation_steps=int(math.ceil(test_batches.n/test_batches.batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit_generator(batches, steps_per_epoch=int(math.ceil(batches.n / batches.batch_size)), epochs=18, verbose=2,\n",
    "                 validation_data=test_batches, validation_steps=int(math.ceil(test_batches.n/test_batches.batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_model():\n",
    "    #model = get_model_bn_dropout()\n",
    "    model = get_model_bn()\n",
    "    model.fit_generator(batches, steps_per_epoch=int(math.ceil(batches.n / batches.batch_size)), epochs=1, verbose=2,\n",
    "                 validation_data=test_batches, validation_steps=int(math.ceil(test_batches.n/test_batches.batch_size)))\n",
    "    model.optimizer.lr = 0.1\n",
    "    model.fit_generator(batches, steps_per_epoch=int(math.ceil(batches.n / batches.batch_size)), epochs=4, verbose=2,\n",
    "                 validation_data=test_batches, validation_steps=int(math.ceil(test_batches.n/test_batches.batch_size)))\n",
    "    model.optimizer.lr = 0.01\n",
    "    model.fit_generator(batches, steps_per_epoch=int(math.ceil(batches.n / batches.batch_size)), epochs=12, verbose=2,\n",
    "                 validation_data=test_batches, validation_steps=int(math.ceil(test_batches.n/test_batches.batch_size)))\n",
    "    model.optimizer.lr = 0.001\n",
    "    model.fit_generator(batches, steps_per_epoch=int(math.ceil(batches.n / batches.batch_size)), epochs=18, verbose=2,\n",
    "                 validation_data=test_batches, validation_steps=int(math.ceil(test_batches.n/test_batches.batch_size)))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      " - 104s - loss: 0.1608 - acc: 0.9496 - val_loss: 0.0338 - val_acc: 0.9893\n",
      "Epoch 1/4\n",
      " - 102s - loss: 0.0701 - acc: 0.9782 - val_loss: 0.0223 - val_acc: 0.9927\n",
      "Epoch 2/4\n",
      " - 102s - loss: 0.0581 - acc: 0.9822 - val_loss: 0.0345 - val_acc: 0.9882\n",
      "Epoch 3/4\n",
      " - 103s - loss: 0.0541 - acc: 0.9842 - val_loss: 0.0211 - val_acc: 0.9929\n",
      "Epoch 4/4\n",
      " - 103s - loss: 0.0461 - acc: 0.9854 - val_loss: 0.0237 - val_acc: 0.9923\n",
      "Epoch 1/12\n",
      " - 103s - loss: 0.0439 - acc: 0.9861 - val_loss: 0.0226 - val_acc: 0.9927\n",
      "Epoch 2/12\n",
      " - 102s - loss: 0.0405 - acc: 0.9875 - val_loss: 0.0236 - val_acc: 0.9923\n",
      "Epoch 3/12\n",
      " - 103s - loss: 0.0384 - acc: 0.9879 - val_loss: 0.0206 - val_acc: 0.9934\n",
      "Epoch 4/12\n",
      " - 102s - loss: 0.0355 - acc: 0.9890 - val_loss: 0.0147 - val_acc: 0.9954\n",
      "Epoch 5/12\n",
      " - 103s - loss: 0.0333 - acc: 0.9893 - val_loss: 0.0197 - val_acc: 0.9936\n",
      "Epoch 6/12\n",
      " - 103s - loss: 0.0320 - acc: 0.9899 - val_loss: 0.0166 - val_acc: 0.9949\n",
      "Epoch 7/12\n",
      " - 103s - loss: 0.0322 - acc: 0.9901 - val_loss: 0.0148 - val_acc: 0.9948\n",
      "Epoch 8/12\n",
      " - 102s - loss: 0.0285 - acc: 0.9907 - val_loss: 0.0144 - val_acc: 0.9950\n",
      "Epoch 9/12\n",
      " - 103s - loss: 0.0280 - acc: 0.9914 - val_loss: 0.0137 - val_acc: 0.9959\n",
      "Epoch 10/12\n",
      " - 102s - loss: 0.0252 - acc: 0.9918 - val_loss: 0.0202 - val_acc: 0.9936\n",
      "Epoch 11/12\n",
      " - 103s - loss: 0.0269 - acc: 0.9918 - val_loss: 0.0126 - val_acc: 0.9959\n",
      "Epoch 12/12\n",
      " - 103s - loss: 0.0262 - acc: 0.9917 - val_loss: 0.0168 - val_acc: 0.9956\n",
      "Epoch 1/18\n",
      " - 103s - loss: 0.0256 - acc: 0.9919 - val_loss: 0.0139 - val_acc: 0.9953\n",
      "Epoch 2/18\n",
      " - 103s - loss: 0.0235 - acc: 0.9926 - val_loss: 0.0152 - val_acc: 0.9946\n",
      "Epoch 3/18\n",
      " - 103s - loss: 0.0241 - acc: 0.9923 - val_loss: 0.0142 - val_acc: 0.9951\n",
      "Epoch 4/18\n",
      " - 103s - loss: 0.0231 - acc: 0.9927 - val_loss: 0.0116 - val_acc: 0.9963\n",
      "Epoch 5/18\n",
      " - 103s - loss: 0.0222 - acc: 0.9929 - val_loss: 0.0141 - val_acc: 0.9953\n",
      "Epoch 6/18\n",
      " - 103s - loss: 0.0220 - acc: 0.9931 - val_loss: 0.0142 - val_acc: 0.9953\n",
      "Epoch 7/18\n",
      " - 102s - loss: 0.0197 - acc: 0.9936 - val_loss: 0.0174 - val_acc: 0.9939\n",
      "Epoch 8/18\n",
      " - 103s - loss: 0.0201 - acc: 0.9937 - val_loss: 0.0115 - val_acc: 0.9964\n",
      "Epoch 9/18\n",
      " - 103s - loss: 0.0194 - acc: 0.9936 - val_loss: 0.0139 - val_acc: 0.9958\n",
      "Epoch 10/18\n",
      " - 103s - loss: 0.0202 - acc: 0.9936 - val_loss: 0.0167 - val_acc: 0.9951\n",
      "Epoch 11/18\n",
      " - 103s - loss: 0.0179 - acc: 0.9940 - val_loss: 0.0122 - val_acc: 0.9957\n",
      "Epoch 12/18\n",
      " - 103s - loss: 0.0174 - acc: 0.9941 - val_loss: 0.0142 - val_acc: 0.9948\n",
      "Epoch 13/18\n",
      " - 103s - loss: 0.0189 - acc: 0.9936 - val_loss: 0.0131 - val_acc: 0.9962\n",
      "Epoch 14/18\n",
      " - 103s - loss: 0.0197 - acc: 0.9938 - val_loss: 0.0111 - val_acc: 0.9965\n",
      "Epoch 15/18\n",
      " - 103s - loss: 0.0166 - acc: 0.9944 - val_loss: 0.0172 - val_acc: 0.9951\n",
      "Epoch 16/18\n",
      " - 102s - loss: 0.0170 - acc: 0.9948 - val_loss: 0.0112 - val_acc: 0.9962\n",
      "Epoch 17/18\n",
      " - 103s - loss: 0.0166 - acc: 0.9946 - val_loss: 0.0116 - val_acc: 0.9958\n",
      "Epoch 18/18\n",
      " - 103s - loss: 0.0172 - acc: 0.9943 - val_loss: 0.0127 - val_acc: 0.9962\n",
      "Epoch 1/1\n",
      " - 104s - loss: 0.1583 - acc: 0.9503 - val_loss: 0.0269 - val_acc: 0.9906\n",
      "Epoch 1/4\n",
      " - 102s - loss: 0.0699 - acc: 0.9780 - val_loss: 0.0258 - val_acc: 0.9911\n",
      "Epoch 2/4\n",
      " - 103s - loss: 0.0583 - acc: 0.9817 - val_loss: 0.0230 - val_acc: 0.9923\n",
      "Epoch 3/4\n",
      " - 102s - loss: 0.0513 - acc: 0.9836 - val_loss: 0.0186 - val_acc: 0.9931\n",
      "Epoch 4/4\n",
      " - 102s - loss: 0.0479 - acc: 0.9854 - val_loss: 0.0229 - val_acc: 0.9928\n",
      "Epoch 1/12\n",
      " - 102s - loss: 0.0428 - acc: 0.9866 - val_loss: 0.0212 - val_acc: 0.9932\n",
      "Epoch 2/12\n",
      " - 102s - loss: 0.0427 - acc: 0.9862 - val_loss: 0.0196 - val_acc: 0.9938\n",
      "Epoch 3/12\n",
      " - 102s - loss: 0.0370 - acc: 0.9881 - val_loss: 0.0229 - val_acc: 0.9928\n",
      "Epoch 4/12\n",
      " - 102s - loss: 0.0362 - acc: 0.9889 - val_loss: 0.0160 - val_acc: 0.9947\n",
      "Epoch 5/12\n",
      " - 102s - loss: 0.0356 - acc: 0.9892 - val_loss: 0.0174 - val_acc: 0.9948\n",
      "Epoch 6/12\n",
      " - 103s - loss: 0.0331 - acc: 0.9896 - val_loss: 0.0207 - val_acc: 0.9931\n",
      "Epoch 7/12\n",
      " - 103s - loss: 0.0304 - acc: 0.9903 - val_loss: 0.0205 - val_acc: 0.9935\n",
      "Epoch 8/12\n",
      " - 102s - loss: 0.0284 - acc: 0.9913 - val_loss: 0.0233 - val_acc: 0.9933\n",
      "Epoch 9/12\n",
      " - 103s - loss: 0.0310 - acc: 0.9906 - val_loss: 0.0171 - val_acc: 0.9932\n",
      "Epoch 10/12\n",
      " - 102s - loss: 0.0269 - acc: 0.9918 - val_loss: 0.0154 - val_acc: 0.9948\n",
      "Epoch 11/12\n",
      " - 103s - loss: 0.0268 - acc: 0.9914 - val_loss: 0.0167 - val_acc: 0.9944\n",
      "Epoch 12/12\n",
      " - 102s - loss: 0.0247 - acc: 0.9922 - val_loss: 0.0142 - val_acc: 0.9956\n",
      "Epoch 1/18\n",
      " - 103s - loss: 0.0231 - acc: 0.9926 - val_loss: 0.0132 - val_acc: 0.9952\n",
      "Epoch 2/18\n",
      " - 102s - loss: 0.0251 - acc: 0.9922 - val_loss: 0.0137 - val_acc: 0.9955\n",
      "Epoch 3/18\n",
      " - 102s - loss: 0.0218 - acc: 0.9928 - val_loss: 0.0186 - val_acc: 0.9947\n",
      "Epoch 4/18\n",
      " - 102s - loss: 0.0243 - acc: 0.9920 - val_loss: 0.0131 - val_acc: 0.9963\n",
      "Epoch 5/18\n",
      " - 102s - loss: 0.0218 - acc: 0.9926 - val_loss: 0.0184 - val_acc: 0.9950\n",
      "Epoch 6/18\n",
      " - 102s - loss: 0.0208 - acc: 0.9930 - val_loss: 0.0178 - val_acc: 0.9944\n",
      "Epoch 7/18\n",
      " - 102s - loss: 0.0199 - acc: 0.9936 - val_loss: 0.0108 - val_acc: 0.9963\n",
      "Epoch 8/18\n",
      " - 103s - loss: 0.0199 - acc: 0.9935 - val_loss: 0.0158 - val_acc: 0.9951\n",
      "Epoch 9/18\n",
      " - 102s - loss: 0.0201 - acc: 0.9939 - val_loss: 0.0173 - val_acc: 0.9949\n",
      "Epoch 10/18\n",
      " - 103s - loss: 0.0203 - acc: 0.9936 - val_loss: 0.0122 - val_acc: 0.9957\n",
      "Epoch 11/18\n",
      " - 102s - loss: 0.0196 - acc: 0.9941 - val_loss: 0.0150 - val_acc: 0.9956\n",
      "Epoch 12/18\n",
      " - 103s - loss: 0.0192 - acc: 0.9939 - val_loss: 0.0146 - val_acc: 0.9948\n",
      "Epoch 13/18\n",
      " - 102s - loss: 0.0188 - acc: 0.9940 - val_loss: 0.0120 - val_acc: 0.9958\n",
      "Epoch 14/18\n",
      " - 102s - loss: 0.0184 - acc: 0.9941 - val_loss: 0.0146 - val_acc: 0.9946\n",
      "Epoch 15/18\n",
      " - 102s - loss: 0.0170 - acc: 0.9944 - val_loss: 0.0166 - val_acc: 0.9949\n",
      "Epoch 16/18\n",
      " - 102s - loss: 0.0183 - acc: 0.9941 - val_loss: 0.0214 - val_acc: 0.9944\n",
      "Epoch 17/18\n",
      " - 102s - loss: 0.0161 - acc: 0.9950 - val_loss: 0.0133 - val_acc: 0.9961\n",
      "Epoch 18/18\n",
      " - 102s - loss: 0.0174 - acc: 0.9946 - val_loss: 0.0150 - val_acc: 0.9950\n",
      "Epoch 1/1\n",
      " - 105s - loss: 0.1610 - acc: 0.9503 - val_loss: 0.0262 - val_acc: 0.9914\n",
      "Epoch 1/4\n",
      " - 102s - loss: 0.0695 - acc: 0.9786 - val_loss: 0.0241 - val_acc: 0.9921\n",
      "Epoch 2/4\n",
      " - 103s - loss: 0.0586 - acc: 0.9819 - val_loss: 0.0295 - val_acc: 0.9897\n",
      "Epoch 3/4\n",
      " - 102s - loss: 0.0548 - acc: 0.9825 - val_loss: 0.0163 - val_acc: 0.9949\n",
      "Epoch 4/4\n",
      " - 103s - loss: 0.0500 - acc: 0.9845 - val_loss: 0.0168 - val_acc: 0.9944\n",
      "Epoch 1/12\n",
      " - 102s - loss: 0.0449 - acc: 0.9862 - val_loss: 0.0224 - val_acc: 0.9926\n",
      "Epoch 2/12\n",
      " - 102s - loss: 0.0432 - acc: 0.9863 - val_loss: 0.0178 - val_acc: 0.9940\n",
      "Epoch 3/12\n",
      " - 102s - loss: 0.0380 - acc: 0.9880 - val_loss: 0.0227 - val_acc: 0.9924\n",
      "Epoch 4/12\n",
      " - 102s - loss: 0.0383 - acc: 0.9881 - val_loss: 0.0182 - val_acc: 0.9948\n",
      "Epoch 5/12\n",
      " - 102s - loss: 0.0334 - acc: 0.9893 - val_loss: 0.0128 - val_acc: 0.9960\n",
      "Epoch 6/12\n",
      " - 102s - loss: 0.0334 - acc: 0.9898 - val_loss: 0.0188 - val_acc: 0.9942\n",
      "Epoch 7/12\n",
      " - 102s - loss: 0.0299 - acc: 0.9903 - val_loss: 0.0125 - val_acc: 0.9958\n",
      "Epoch 8/12\n",
      " - 103s - loss: 0.0287 - acc: 0.9908 - val_loss: 0.0170 - val_acc: 0.9945\n",
      "Epoch 9/12\n",
      " - 103s - loss: 0.0290 - acc: 0.9908 - val_loss: 0.0152 - val_acc: 0.9950\n",
      "Epoch 10/12\n",
      " - 102s - loss: 0.0278 - acc: 0.9910 - val_loss: 0.0158 - val_acc: 0.9950\n",
      "Epoch 11/12\n",
      " - 103s - loss: 0.0271 - acc: 0.9915 - val_loss: 0.0124 - val_acc: 0.9956\n",
      "Epoch 12/12\n",
      " - 102s - loss: 0.0255 - acc: 0.9922 - val_loss: 0.0162 - val_acc: 0.9947\n",
      "Epoch 1/18\n",
      " - 103s - loss: 0.0255 - acc: 0.9919 - val_loss: 0.0153 - val_acc: 0.9947\n",
      "Epoch 2/18\n",
      " - 102s - loss: 0.0246 - acc: 0.9922 - val_loss: 0.0120 - val_acc: 0.9958\n",
      "Epoch 3/18\n",
      " - 103s - loss: 0.0252 - acc: 0.9925 - val_loss: 0.0133 - val_acc: 0.9961\n",
      "Epoch 4/18\n",
      " - 102s - loss: 0.0229 - acc: 0.9927 - val_loss: 0.0131 - val_acc: 0.9951\n",
      "Epoch 5/18\n",
      " - 103s - loss: 0.0218 - acc: 0.9931 - val_loss: 0.0135 - val_acc: 0.9957\n",
      "Epoch 6/18\n",
      " - 102s - loss: 0.0200 - acc: 0.9935 - val_loss: 0.0153 - val_acc: 0.9950\n",
      "Epoch 7/18\n",
      " - 103s - loss: 0.0219 - acc: 0.9931 - val_loss: 0.0176 - val_acc: 0.9952\n",
      "Epoch 8/18\n",
      " - 102s - loss: 0.0204 - acc: 0.9935 - val_loss: 0.0159 - val_acc: 0.9946\n",
      "Epoch 9/18\n",
      " - 102s - loss: 0.0198 - acc: 0.9933 - val_loss: 0.0168 - val_acc: 0.9942\n",
      "Epoch 10/18\n",
      " - 102s - loss: 0.0207 - acc: 0.9935 - val_loss: 0.0161 - val_acc: 0.9949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/18\n",
      " - 102s - loss: 0.0186 - acc: 0.9942 - val_loss: 0.0154 - val_acc: 0.9946\n",
      "Epoch 12/18\n",
      " - 102s - loss: 0.0175 - acc: 0.9942 - val_loss: 0.0144 - val_acc: 0.9952\n",
      "Epoch 13/18\n",
      " - 102s - loss: 0.0185 - acc: 0.9941 - val_loss: 0.0139 - val_acc: 0.9958\n",
      "Epoch 14/18\n",
      " - 102s - loss: 0.0183 - acc: 0.9939 - val_loss: 0.0117 - val_acc: 0.9957\n",
      "Epoch 15/18\n",
      " - 102s - loss: 0.0190 - acc: 0.9939 - val_loss: 0.0137 - val_acc: 0.9957\n",
      "Epoch 16/18\n",
      " - 102s - loss: 0.0158 - acc: 0.9947 - val_loss: 0.0143 - val_acc: 0.9955\n",
      "Epoch 17/18\n",
      " - 102s - loss: 0.0173 - acc: 0.9945 - val_loss: 0.0137 - val_acc: 0.9960\n",
      "Epoch 18/18\n",
      " - 102s - loss: 0.0166 - acc: 0.9948 - val_loss: 0.0139 - val_acc: 0.9954\n",
      "Epoch 1/1\n",
      " - 104s - loss: 0.1625 - acc: 0.9495 - val_loss: 0.0297 - val_acc: 0.9906\n",
      "Epoch 1/4\n",
      " - 103s - loss: 0.0694 - acc: 0.9784 - val_loss: 0.0265 - val_acc: 0.9912\n",
      "Epoch 2/4\n",
      " - 102s - loss: 0.0611 - acc: 0.9805 - val_loss: 0.0230 - val_acc: 0.9921\n",
      "Epoch 3/4\n",
      " - 102s - loss: 0.0527 - acc: 0.9839 - val_loss: 0.0255 - val_acc: 0.9921\n",
      "Epoch 4/4\n",
      " - 103s - loss: 0.0483 - acc: 0.9851 - val_loss: 0.0239 - val_acc: 0.9918\n",
      "Epoch 1/12\n",
      " - 102s - loss: 0.0423 - acc: 0.9861 - val_loss: 0.0241 - val_acc: 0.9929\n",
      "Epoch 2/12\n",
      " - 103s - loss: 0.0426 - acc: 0.9870 - val_loss: 0.0173 - val_acc: 0.9945\n",
      "Epoch 3/12\n",
      " - 102s - loss: 0.0385 - acc: 0.9877 - val_loss: 0.0223 - val_acc: 0.9935\n",
      "Epoch 4/12\n",
      " - 103s - loss: 0.0357 - acc: 0.9888 - val_loss: 0.0232 - val_acc: 0.9925\n",
      "Epoch 5/12\n",
      " - 103s - loss: 0.0351 - acc: 0.9890 - val_loss: 0.0154 - val_acc: 0.9955\n",
      "Epoch 6/12\n",
      " - 103s - loss: 0.0336 - acc: 0.9897 - val_loss: 0.0169 - val_acc: 0.9957\n",
      "Epoch 7/12\n",
      " - 102s - loss: 0.0316 - acc: 0.9900 - val_loss: 0.0176 - val_acc: 0.9942\n",
      "Epoch 8/12\n",
      " - 103s - loss: 0.0316 - acc: 0.9900 - val_loss: 0.0161 - val_acc: 0.9944\n",
      "Epoch 9/12\n",
      " - 102s - loss: 0.0285 - acc: 0.9907 - val_loss: 0.0167 - val_acc: 0.9946\n",
      "Epoch 10/12\n",
      " - 103s - loss: 0.0279 - acc: 0.9911 - val_loss: 0.0166 - val_acc: 0.9949\n",
      "Epoch 11/12\n",
      " - 102s - loss: 0.0270 - acc: 0.9915 - val_loss: 0.0165 - val_acc: 0.9949\n",
      "Epoch 12/12\n",
      " - 102s - loss: 0.0242 - acc: 0.9920 - val_loss: 0.0166 - val_acc: 0.9954\n",
      "Epoch 1/18\n",
      " - 102s - loss: 0.0252 - acc: 0.9917 - val_loss: 0.0174 - val_acc: 0.9946\n",
      "Epoch 2/18\n",
      " - 102s - loss: 0.0231 - acc: 0.9927 - val_loss: 0.0187 - val_acc: 0.9937\n",
      "Epoch 3/18\n",
      " - 102s - loss: 0.0249 - acc: 0.9920 - val_loss: 0.0158 - val_acc: 0.9947\n",
      "Epoch 4/18\n",
      " - 102s - loss: 0.0226 - acc: 0.9931 - val_loss: 0.0160 - val_acc: 0.9947\n",
      "Epoch 5/18\n",
      " - 103s - loss: 0.0226 - acc: 0.9929 - val_loss: 0.0158 - val_acc: 0.9945\n",
      "Epoch 6/18\n",
      " - 102s - loss: 0.0208 - acc: 0.9934 - val_loss: 0.0212 - val_acc: 0.9937\n",
      "Epoch 7/18\n",
      " - 102s - loss: 0.0224 - acc: 0.9926 - val_loss: 0.0205 - val_acc: 0.9939\n",
      "Epoch 8/18\n",
      " - 102s - loss: 0.0218 - acc: 0.9936 - val_loss: 0.0169 - val_acc: 0.9956\n",
      "Epoch 9/18\n",
      " - 103s - loss: 0.0194 - acc: 0.9939 - val_loss: 0.0153 - val_acc: 0.9950\n",
      "Epoch 10/18\n",
      " - 102s - loss: 0.0184 - acc: 0.9938 - val_loss: 0.0170 - val_acc: 0.9949\n",
      "Epoch 11/18\n",
      " - 102s - loss: 0.0201 - acc: 0.9933 - val_loss: 0.0155 - val_acc: 0.9949\n",
      "Epoch 12/18\n",
      " - 102s - loss: 0.0192 - acc: 0.9936 - val_loss: 0.0155 - val_acc: 0.9945\n",
      "Epoch 13/18\n",
      " - 103s - loss: 0.0166 - acc: 0.9946 - val_loss: 0.0153 - val_acc: 0.9953\n",
      "Epoch 14/18\n",
      " - 102s - loss: 0.0174 - acc: 0.9942 - val_loss: 0.0151 - val_acc: 0.9948\n",
      "Epoch 15/18\n",
      " - 102s - loss: 0.0178 - acc: 0.9943 - val_loss: 0.0153 - val_acc: 0.9954\n",
      "Epoch 16/18\n",
      " - 102s - loss: 0.0177 - acc: 0.9944 - val_loss: 0.0171 - val_acc: 0.9943\n",
      "Epoch 17/18\n",
      " - 102s - loss: 0.0181 - acc: 0.9943 - val_loss: 0.0143 - val_acc: 0.9948\n",
      "Epoch 18/18\n",
      " - 102s - loss: 0.0159 - acc: 0.9950 - val_loss: 0.0157 - val_acc: 0.9953\n",
      "Epoch 1/1\n",
      " - 104s - loss: 0.1605 - acc: 0.9496 - val_loss: 0.0334 - val_acc: 0.9887\n",
      "Epoch 1/4\n",
      " - 103s - loss: 0.0716 - acc: 0.9775 - val_loss: 0.0287 - val_acc: 0.9909\n",
      "Epoch 2/4\n",
      " - 102s - loss: 0.0592 - acc: 0.9815 - val_loss: 0.0204 - val_acc: 0.9936\n",
      "Epoch 3/4\n",
      " - 103s - loss: 0.0532 - acc: 0.9833 - val_loss: 0.0275 - val_acc: 0.9911\n",
      "Epoch 4/4\n",
      " - 103s - loss: 0.0476 - acc: 0.9848 - val_loss: 0.0268 - val_acc: 0.9920\n",
      "Epoch 1/12\n",
      " - 103s - loss: 0.0438 - acc: 0.9865 - val_loss: 0.0198 - val_acc: 0.9938\n",
      "Epoch 2/12\n",
      " - 103s - loss: 0.0415 - acc: 0.9871 - val_loss: 0.0185 - val_acc: 0.9940\n",
      "Epoch 3/12\n",
      " - 103s - loss: 0.0399 - acc: 0.9879 - val_loss: 0.0190 - val_acc: 0.9940\n",
      "Epoch 4/12\n",
      " - 103s - loss: 0.0372 - acc: 0.9886 - val_loss: 0.0180 - val_acc: 0.9948\n",
      "Epoch 5/12\n",
      " - 102s - loss: 0.0360 - acc: 0.9889 - val_loss: 0.0166 - val_acc: 0.9951\n",
      "Epoch 6/12\n",
      " - 103s - loss: 0.0315 - acc: 0.9903 - val_loss: 0.0218 - val_acc: 0.9938\n",
      "Epoch 7/12\n",
      " - 102s - loss: 0.0313 - acc: 0.9902 - val_loss: 0.0181 - val_acc: 0.9946\n",
      "Epoch 8/12\n",
      " - 103s - loss: 0.0307 - acc: 0.9900 - val_loss: 0.0219 - val_acc: 0.9930\n",
      "Epoch 9/12\n",
      " - 102s - loss: 0.0301 - acc: 0.9903 - val_loss: 0.0189 - val_acc: 0.9940\n",
      "Epoch 10/12\n",
      " - 102s - loss: 0.0272 - acc: 0.9913 - val_loss: 0.0148 - val_acc: 0.9947\n",
      "Epoch 11/12\n",
      " - 103s - loss: 0.0254 - acc: 0.9919 - val_loss: 0.0174 - val_acc: 0.9943\n",
      "Epoch 12/12\n",
      " - 102s - loss: 0.0263 - acc: 0.9919 - val_loss: 0.0199 - val_acc: 0.9948\n",
      "Epoch 1/18\n",
      " - 102s - loss: 0.0249 - acc: 0.9923 - val_loss: 0.0142 - val_acc: 0.9953\n",
      "Epoch 2/18\n",
      " - 103s - loss: 0.0255 - acc: 0.9921 - val_loss: 0.0188 - val_acc: 0.9948\n",
      "Epoch 3/18\n",
      " - 103s - loss: 0.0241 - acc: 0.9928 - val_loss: 0.0187 - val_acc: 0.9942\n",
      "Epoch 4/18\n",
      " - 102s - loss: 0.0236 - acc: 0.9925 - val_loss: 0.0144 - val_acc: 0.9951\n",
      "Epoch 5/18\n",
      " - 103s - loss: 0.0213 - acc: 0.9931 - val_loss: 0.0186 - val_acc: 0.9944\n",
      "Epoch 6/18\n",
      " - 102s - loss: 0.0209 - acc: 0.9932 - val_loss: 0.0160 - val_acc: 0.9949\n",
      "Epoch 7/18\n",
      " - 103s - loss: 0.0204 - acc: 0.9931 - val_loss: 0.0160 - val_acc: 0.9955\n",
      "Epoch 8/18\n",
      " - 102s - loss: 0.0205 - acc: 0.9936 - val_loss: 0.0143 - val_acc: 0.9952\n",
      "Epoch 9/18\n",
      " - 103s - loss: 0.0207 - acc: 0.9933 - val_loss: 0.0124 - val_acc: 0.9959\n",
      "Epoch 10/18\n",
      " - 102s - loss: 0.0183 - acc: 0.9943 - val_loss: 0.0149 - val_acc: 0.9951\n",
      "Epoch 11/18\n",
      " - 103s - loss: 0.0183 - acc: 0.9939 - val_loss: 0.0144 - val_acc: 0.9951\n",
      "Epoch 12/18\n",
      " - 103s - loss: 0.0192 - acc: 0.9937 - val_loss: 0.0152 - val_acc: 0.9955\n",
      "Epoch 13/18\n",
      " - 103s - loss: 0.0188 - acc: 0.9940 - val_loss: 0.0144 - val_acc: 0.9954\n",
      "Epoch 14/18\n",
      " - 102s - loss: 0.0178 - acc: 0.9941 - val_loss: 0.0138 - val_acc: 0.9959\n",
      "Epoch 15/18\n",
      " - 103s - loss: 0.0172 - acc: 0.9945 - val_loss: 0.0146 - val_acc: 0.9956\n",
      "Epoch 16/18\n",
      " - 103s - loss: 0.0177 - acc: 0.9945 - val_loss: 0.0176 - val_acc: 0.9948\n",
      "Epoch 17/18\n",
      " - 102s - loss: 0.0178 - acc: 0.9946 - val_loss: 0.0139 - val_acc: 0.9965\n",
      "Epoch 18/18\n",
      " - 103s - loss: 0.0160 - acc: 0.9950 - val_loss: 0.0148 - val_acc: 0.9953\n",
      "Epoch 1/1\n",
      " - 104s - loss: 0.1613 - acc: 0.9493 - val_loss: 0.0380 - val_acc: 0.9877\n",
      "Epoch 1/4\n",
      " - 103s - loss: 0.0711 - acc: 0.9780 - val_loss: 0.0217 - val_acc: 0.9931\n",
      "Epoch 2/4\n",
      " - 102s - loss: 0.0556 - acc: 0.9824 - val_loss: 0.0270 - val_acc: 0.9918\n",
      "Epoch 3/4\n",
      " - 103s - loss: 0.0526 - acc: 0.9832 - val_loss: 0.0255 - val_acc: 0.9910\n",
      "Epoch 4/4\n",
      " - 102s - loss: 0.0470 - acc: 0.9853 - val_loss: 0.0188 - val_acc: 0.9943\n",
      "Epoch 1/12\n",
      " - 103s - loss: 0.0439 - acc: 0.9862 - val_loss: 0.0239 - val_acc: 0.9930\n",
      "Epoch 2/12\n",
      " - 102s - loss: 0.0418 - acc: 0.9866 - val_loss: 0.0225 - val_acc: 0.9921\n",
      "Epoch 3/12\n",
      " - 103s - loss: 0.0385 - acc: 0.9875 - val_loss: 0.0205 - val_acc: 0.9936\n",
      "Epoch 4/12\n",
      " - 103s - loss: 0.0354 - acc: 0.9888 - val_loss: 0.0164 - val_acc: 0.9945\n",
      "Epoch 5/12\n",
      " - 102s - loss: 0.0336 - acc: 0.9898 - val_loss: 0.0155 - val_acc: 0.9949\n",
      "Epoch 6/12\n",
      " - 103s - loss: 0.0339 - acc: 0.9892 - val_loss: 0.0422 - val_acc: 0.9876\n",
      "Epoch 7/12\n",
      " - 102s - loss: 0.0308 - acc: 0.9899 - val_loss: 0.0149 - val_acc: 0.9955\n",
      "Epoch 8/12\n",
      " - 103s - loss: 0.0294 - acc: 0.9908 - val_loss: 0.0164 - val_acc: 0.9944\n",
      "Epoch 9/12\n",
      " - 102s - loss: 0.0301 - acc: 0.9904 - val_loss: 0.0184 - val_acc: 0.9943\n",
      "Epoch 10/12\n",
      " - 102s - loss: 0.0282 - acc: 0.9912 - val_loss: 0.0171 - val_acc: 0.9949\n",
      "Epoch 11/12\n",
      " - 102s - loss: 0.0258 - acc: 0.9917 - val_loss: 0.0160 - val_acc: 0.9955\n",
      "Epoch 12/12\n",
      " - 103s - loss: 0.0252 - acc: 0.9919 - val_loss: 0.0129 - val_acc: 0.9962\n",
      "Epoch 1/18\n",
      " - 102s - loss: 0.0238 - acc: 0.9923 - val_loss: 0.0153 - val_acc: 0.9948\n",
      "Epoch 2/18\n",
      " - 103s - loss: 0.0236 - acc: 0.9924 - val_loss: 0.0159 - val_acc: 0.9955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/18\n",
      " - 103s - loss: 0.0233 - acc: 0.9927 - val_loss: 0.0155 - val_acc: 0.9947\n",
      "Epoch 4/18\n",
      " - 103s - loss: 0.0229 - acc: 0.9928 - val_loss: 0.0160 - val_acc: 0.9958\n",
      "Epoch 5/18\n",
      " - 103s - loss: 0.0231 - acc: 0.9922 - val_loss: 0.0132 - val_acc: 0.9958\n",
      "Epoch 6/18\n",
      " - 102s - loss: 0.0220 - acc: 0.9932 - val_loss: 0.0156 - val_acc: 0.9951\n",
      "Epoch 7/18\n",
      " - 102s - loss: 0.0205 - acc: 0.9937 - val_loss: 0.0135 - val_acc: 0.9961\n",
      "Epoch 8/18\n",
      " - 102s - loss: 0.0184 - acc: 0.9943 - val_loss: 0.0133 - val_acc: 0.9953\n",
      "Epoch 9/18\n",
      " - 103s - loss: 0.0209 - acc: 0.9936 - val_loss: 0.0159 - val_acc: 0.9954\n",
      "Epoch 10/18\n",
      " - 103s - loss: 0.0199 - acc: 0.9938 - val_loss: 0.0161 - val_acc: 0.9953\n",
      "Epoch 11/18\n",
      " - 103s - loss: 0.0203 - acc: 0.9934 - val_loss: 0.0129 - val_acc: 0.9959\n",
      "Epoch 12/18\n",
      " - 102s - loss: 0.0194 - acc: 0.9936 - val_loss: 0.0138 - val_acc: 0.9955\n",
      "Epoch 13/18\n",
      " - 103s - loss: 0.0189 - acc: 0.9941 - val_loss: 0.0178 - val_acc: 0.9957\n",
      "Epoch 14/18\n",
      " - 102s - loss: 0.0187 - acc: 0.9941 - val_loss: 0.0153 - val_acc: 0.9958\n",
      "Epoch 15/18\n",
      " - 102s - loss: 0.0193 - acc: 0.9940 - val_loss: 0.0139 - val_acc: 0.9958\n",
      "Epoch 16/18\n",
      " - 103s - loss: 0.0169 - acc: 0.9943 - val_loss: 0.0124 - val_acc: 0.9961\n",
      "Epoch 17/18\n",
      " - 103s - loss: 0.0172 - acc: 0.9947 - val_loss: 0.0150 - val_acc: 0.9955\n",
      "Epoch 18/18\n",
      " - 103s - loss: 0.0188 - acc: 0.9943 - val_loss: 0.0145 - val_acc: 0.9952\n"
     ]
    }
   ],
   "source": [
    "models = [fit_model() for i in range(6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/mnist/'\n",
    "model_path = path + 'models/'\n",
    "if not os.path.exists(path): os.mkdir(path)\n",
    "if not os.path.exists(model_path): os.mkdir(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, mdl in enumerate(models):\n",
    "    mdl.save_weights(model_path+'cnn-mnist23-'+str(i)+'.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Weights\n",
    "\"\"\"\n",
    "models = []\n",
    "for i in range(6):\n",
    "    model = get_model_bn()\n",
    "    model.load_weights(model_path+'cnn-mnist23-'+str(i)+'.pkl')\n",
    "    models.append(model)\n",
    "models = np.array(models)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  256/10000 [..............................] - ETA: 21s\n",
      "  768/10000 [=>............................] - ETA: 7s \n",
      " 1280/10000 [==>...........................] - ETA: 4s\n",
      " 1792/10000 [====>.........................] - ETA: 3s\n",
      " 2560/10000 [======>.......................] - ETA: 2s\n",
      " 3328/10000 [========>.....................] - ETA: 1s\n",
      " 4096/10000 [===========>..................] - ETA: 1s\n",
      " 4864/10000 [=============>................] - ETA: 1s\n",
      " 5632/10000 [===============>..............] - ETA: 0s\n",
      " 6400/10000 [==================>...........] - ETA: 0s\n",
      " 7168/10000 [====================>.........] - ETA: 0s\n",
      " 7936/10000 [======================>.......] - ETA: 0s\n",
      " 8704/10000 [=========================>....] - ETA: 0s\n",
      " 9472/10000 [===========================>..] - ETA: 0s\n",
      "10000/10000 [==============================] - 1s 141us/step\n",
      "\n",
      "  256/10000 [..............................] - ETA: 4s\n",
      " 1024/10000 [==>...........................] - ETA: 1s\n",
      " 1792/10000 [====>.........................] - ETA: 1s\n",
      " 2560/10000 [======>.......................] - ETA: 0s\n",
      " 3328/10000 [========>.....................] - ETA: 0s\n",
      " 4096/10000 [===========>..................] - ETA: 0s\n",
      " 4864/10000 [=============>................] - ETA: 0s\n",
      " 5632/10000 [===============>..............] - ETA: 0s\n",
      " 6400/10000 [==================>...........] - ETA: 0s\n",
      " 7168/10000 [====================>.........] - ETA: 0s\n",
      " 7936/10000 [======================>.......] - ETA: 0s\n",
      " 8704/10000 [=========================>....] - ETA: 0s\n",
      " 9472/10000 [===========================>..] - ETA: 0s\n",
      "10000/10000 [==============================] - 1s 81us/step\n",
      "\n",
      "  256/10000 [..............................] - ETA: 4s\n",
      " 1024/10000 [==>...........................] - ETA: 1s\n",
      " 1792/10000 [====>.........................] - ETA: 1s\n",
      " 2560/10000 [======>.......................] - ETA: 0s\n",
      " 3328/10000 [========>.....................] - ETA: 0s\n",
      " 4096/10000 [===========>..................] - ETA: 0s\n",
      " 4864/10000 [=============>................] - ETA: 0s\n",
      " 5632/10000 [===============>..............] - ETA: 0s\n",
      " 6400/10000 [==================>...........] - ETA: 0s\n",
      " 7168/10000 [====================>.........] - ETA: 0s\n",
      " 7936/10000 [======================>.......] - ETA: 0s\n",
      " 8704/10000 [=========================>....] - ETA: 0s\n",
      " 9472/10000 [===========================>..] - ETA: 0s\n",
      "10000/10000 [==============================] - 1s 81us/step\n",
      "\n",
      "  256/10000 [..............................] - ETA: 4s\n",
      " 1024/10000 [==>...........................] - ETA: 1s\n",
      " 1792/10000 [====>.........................] - ETA: 1s\n",
      " 2560/10000 [======>.......................] - ETA: 0s\n",
      " 3328/10000 [========>.....................] - ETA: 0s\n",
      " 4096/10000 [===========>..................] - ETA: 0s\n",
      " 4864/10000 [=============>................] - ETA: 0s\n",
      " 5632/10000 [===============>..............] - ETA: 0s\n",
      " 6400/10000 [==================>...........] - ETA: 0s\n",
      " 7168/10000 [====================>.........] - ETA: 0s\n",
      " 7936/10000 [======================>.......] - ETA: 0s\n",
      " 8704/10000 [=========================>....] - ETA: 0s\n",
      " 9472/10000 [===========================>..] - ETA: 0s\n",
      "10000/10000 [==============================] - 1s 81us/step\n",
      "\n",
      "  256/10000 [..............................] - ETA: 4s\n",
      " 1024/10000 [==>...........................] - ETA: 1s\n",
      " 1792/10000 [====>.........................] - ETA: 1s\n",
      " 2560/10000 [======>.......................] - ETA: 0s\n",
      " 3328/10000 [========>.....................] - ETA: 0s\n",
      " 4096/10000 [===========>..................] - ETA: 0s\n",
      " 4864/10000 [=============>................] - ETA: 0s\n",
      " 5632/10000 [===============>..............] - ETA: 0s\n",
      " 6400/10000 [==================>...........] - ETA: 0s\n",
      " 7168/10000 [====================>.........] - ETA: 0s\n",
      " 7936/10000 [======================>.......] - ETA: 0s\n",
      " 8704/10000 [=========================>....] - ETA: 0s\n",
      " 9472/10000 [===========================>..] - ETA: 0s\n",
      "10000/10000 [==============================] - 1s 81us/step\n",
      "\n",
      "  256/10000 [..............................] - ETA: 4s\n",
      " 1024/10000 [==>...........................] - ETA: 1s\n",
      " 1792/10000 [====>.........................] - ETA: 1s\n",
      " 2560/10000 [======>.......................] - ETA: 0s\n",
      " 3328/10000 [========>.....................] - ETA: 0s\n",
      " 4096/10000 [===========>..................] - ETA: 0s\n",
      " 4864/10000 [=============>................] - ETA: 0s\n",
      " 5632/10000 [===============>..............] - ETA: 0s\n",
      " 6400/10000 [==================>...........] - ETA: 0s\n",
      " 7168/10000 [====================>.........] - ETA: 0s\n",
      " 7936/10000 [======================>.......] - ETA: 0s\n",
      " 8704/10000 [=========================>....] - ETA: 0s\n",
      " 9472/10000 [===========================>..] - ETA: 0s\n",
      "10000/10000 [==============================] - 1s 81us/step\n"
     ]
    }
   ],
   "source": [
    "evals = np.array([mdl.evaluate(X_test, y_test, batch_size=batch_size*4) for mdl in models])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0144,  0.9954])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evals.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_preds = np.stack([mdl.predict(X_test, batch_size=batch_size*4) for mdl in models])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 10000, 10)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_preds = all_preds.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99680001"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#keras.metrics.categorical_accuracy(y_test, avg_preds).eval()\n",
    "\n",
    "# https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html\n",
    "from keras.metrics import categorical_accuracy as accuracy\n",
    "\n",
    "acc_value = accuracy(y_test, avg_preds)\n",
    "with sess.as_default():\n",
    "    eval_result = acc_value.eval()\n",
    "    \n",
    "eval_result.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
